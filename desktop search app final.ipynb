{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files found in D:\\ are follwoing:  \n",
      "D:\\ineuron\\data analysis\\drive-download-20210309T052058Z-001\\pandas practice (1).pdf\n",
      "D:\\ineuron\\data analysis\\drive-download-20210309T052058Z-001\\pandas practice1 (1).pdf\n",
      "D:\\ineuron\\python assignment\\basic\\Assignment_1 solution.pdf\n",
      "D:\\ineuron\\python assignment\\basic\\Assignment_2 solution.pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\ReviewScrapper.pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\WebScraping(Image).pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\reviewScrapper running\\reviewScrapper\\WebScrapper(Text).pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\updated_reviewScrapper-20210228T133813Z-001\\reviewScrapper\\WebScrapper(Text).pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\updated_reviewScrapper-20210228T133813Z-001\\reviewScrapper-20210228T133813Z-001\\reviewScrapper\\WebScrapper(Text).pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\webscrappper_text-master\\webscrappper_text-master\\reviewScrapper\\WebScrapper(Text).pdf\n",
      "D:\\ineuron\\python project\\drive-download-20210324T134433Z-001\\zomato scrapper\\reviewScrapper\\WebScrapper(Text).pdf\n",
      "D:\\ineuron\\rest api sql & nonsql\\drive-download-20210317T123159Z-001\\19.2.MySQL\\19.2.MySQL\\MYSQL application.pdf\n",
      "b'In [6]:\\n\\nurl = \\'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.ts\\n\\n2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [1]:\\n\\nimport numpy as np\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nIn [7]:\\n\\n# read the above file\\n\\nIn [14]:\\n\\ndf = pd.read_csv(url, sep = \\'\\\\t\\')\\n\\nIn [9]:\\n\\n#print first 5 and last 7 records\\n\\nIn [15]:\\n\\ndf.head(5)\\n\\nOut[15]:\\n\\norder_id quantity\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n2\\n\\n1 Chips and Fresh Tomato Salsa\\n\\nIzze\\n\\nNantucket Nectar\\n\\nChips and Tomatillo-Green\\nChili Salsa\\n\\n1\\n\\n1\\n\\n1\\n\\n2\\n\\nitem_name\\n\\nchoice_description item_price\\n\\nNaN\\n\\n[Clementine]\\n\\n[Apple]\\n\\n$2.39\\n\\n$3.39\\n\\n$3.39\\n\\nNaN\\n\\n$2.39\\n\\nChicken Bowl\\n\\n[Tomatillo-Red Chili Salsa (Hot), [Black\\nBeans...\\n\\n$16.98\\n\\nIn [16]:\\n\\ndf.tail(7)\\n\\nOut[16]:\\n\\norder_id quantity\\n\\nitem_name\\n\\nchoice_description item_price\\n\\n4615\\n\\n1832\\n\\n1 Chicken Soft Tacos\\n\\n[Fresh Tomato Salsa, [Rice, Cheese, Sour\\nCream]]\\n\\n$8.75\\n\\n4616\\n\\n1832\\n\\n4617\\n\\n1833\\n\\n1\\n\\n1\\n\\n1\\n\\n4618\\n\\n1833\\n\\nSteak Burrito\\n\\n4619\\n\\n1834\\n\\n1 Chicken Salad Bowl\\n\\n4620\\n\\n1834\\n\\n1 Chicken Salad Bowl\\n\\n4621\\n\\n1834\\n\\n1 Chicken Salad Bowl\\n\\nChips and\\nGuacamole\\n\\nNaN\\n\\n$4.45\\n\\nSteak Burrito\\n\\n[Fresh Tomato Salsa, [Rice, Black Beans, Sour\\n...\\n\\n[Fresh Tomato Salsa, [Rice, Sour Cream,\\nCheese...\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables,\\nPinto...\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables,\\nLettu...\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables,\\nPinto...\\n\\n$11.75\\n\\n$11.75\\n\\n$11.25\\n\\n$8.75\\n\\n$8.75\\n\\nIn [12]:\\n\\n# print total records and type of variables\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n1/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [17]:\\n\\ndf.info()#\\n \\n# OR\\n \\ndf.shape[0]\\n# 4622 observations\\n\\n<class \\'pandas.core.frame.DataFrame\\'> \\nRangeIndex: 4622 entries, 0 to 4621 \\nData columns (total 5 columns): \\norder_id              4622 non-null int64 \\nquantity              4622 non-null int64 \\nitem_name             4622 non-null object \\nchoice_description    3376 non-null object \\nitem_price            4622 non-null object \\ndtypes: int64(2), object(3) \\nmemory usage: 180.6+ KB \\n\\nOut[17]: 4622\\n\\nIn [19]:\\n\\ndf.columns\\n\\nIn [18]:\\n\\n#Print the name of all the columns.\\n\\nOut[19]: Index([\\'order_id\\', \\'quantity\\', \\'item_name\\', \\'choice_description\\', \\n\\n       \\'item_price\\'], \\n      dtype=\\'object\\')\\n\\nIn [20]:\\n\\n#How is the dataset indexed?\\n\\nIn [21]:\\n\\ndf.index\\n\\nOut[21]: RangeIndex(start=0, stop=4622, step=1)\\n\\nIn [22]:\\n\\n#Which was the most ordered item? and How many items were ordered?\\n\\nIn [23]:\\n\\nc = df.groupby(\\'item_name\\')\\nc = c.sum()\\nc = c.sort_values([\\'quantity\\'], ascending=False)\\nc.head(1)\\n\\nOut[23]:\\n\\norder_id quantity\\n\\nitem_name\\n\\nChicken Bowl\\n\\n713926\\n\\n761\\n\\nIn [24]:\\n\\n#What was the most ordered item in the choice_description column?\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n2/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [25]:\\n\\nc = df.groupby(\\'choice_description\\').sum()\\nc = c.sort_values([\\'quantity\\'], ascending=False)\\nc.head(1)\\n\\nOut[25]:\\n\\norder_id quantity\\n\\nchoice_description\\n\\n[Diet Coke]\\n\\n123455\\n\\n159\\n\\nIn [26]:\\n\\n#Turn the item price into a float\\n\\nIn [27]:\\n\\ndollar = lambda x: float(x[1:-1])\\ndf.item_price = df.item_price.apply(dollar)\\n\\nIn [28]:\\n\\n#How much was the revenue for the period in the dataset?\\n\\nIn [30]:\\n\\nrevenue = (df[\\'quantity\\']* df[\\'item_price\\']).sum()\\n \\nprint(\\'Revenue was: $\\' + str(np.round(revenue,2)))\\n\\nRevenue was: $39237.02 \\n\\nIn [31]:\\n\\n#print a data frame with only two columns item_name and item_price\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n3/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [32]:\\n\\n# delete the duplicates in item_name and quantity\\nfiltered = df.drop_duplicates([\\'item_name\\',\\'quantity\\'])\\n \\n# select only the products with quantity equals to 1\\none_prod = filtered[filtered.quantity == 1]\\n \\n# select only the item_name and item_price columns\\nprice_per_item = one_prod[[\\'item_name\\', \\'item_price\\']]\\n \\n# sort the values from the most to less expensive\\nprice_per_item.sort_values(by = \"item_price\", ascending = False)\\n\\nOut[32]:\\n\\nitem_name item_price\\n\\n606\\n\\n1229\\n\\n1132\\n\\n7\\n\\n168\\n\\n39\\n\\n738\\n\\n186\\n\\n62\\n\\n57\\n\\n250\\n\\n5\\n\\n8\\n\\n554\\n\\n237\\n\\n56\\n\\n92\\n\\n664\\n\\n54\\n\\n21\\n\\n27\\n\\n33\\n\\n11\\n\\n12\\n\\n44\\n\\n3750\\n\\n1653\\n\\n16\\n\\nSteak Salad Bowl\\n\\nBarbacoa Salad Bowl\\n\\nCarnitas Salad Bowl\\n\\nSteak Burrito\\n\\nBarbacoa Crispy Tacos\\n\\nBarbacoa Bowl\\n\\nVeggie Soft Tacos\\n\\nVeggie Salad Bowl\\n\\nVeggie Bowl\\n\\nVeggie Burrito\\n\\nChicken Salad\\n\\nChicken Bowl\\n\\nSteak Soft Tacos\\n\\nCarnitas Crispy Tacos\\n\\nCarnitas Soft Tacos\\n\\nBarbacoa Soft Tacos\\n\\nSteak Crispy Tacos\\n\\nSteak Salad\\n\\nSteak Bowl\\n\\nCarnitas Salad\\n\\nBarbacoa Burrito\\n\\nCarnitas Burrito\\n\\nCarnitas Bowl\\n\\nChicken Crispy Tacos\\n\\nChicken Soft Tacos\\n\\nChicken Salad Bowl\\n\\nVeggie Crispy Tacos\\n\\nChicken Burrito\\n\\n11.89\\n\\n11.89\\n\\n11.89\\n\\n11.75\\n\\n11.75\\n\\n11.75\\n\\n11.25\\n\\n11.25\\n\\n11.25\\n\\n11.25\\n\\n10.98\\n\\n10.98\\n\\n9.25\\n\\n9.25\\n\\n9.25\\n\\n9.25\\n\\n9.25\\n\\n8.99\\n\\n8.99\\n\\n8.99\\n\\n8.99\\n\\n8.99\\n\\n8.99\\n\\n8.75\\n\\n8.75\\n\\n8.75\\n\\n8.49\\n\\n8.49\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n4/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nitem_name item_price\\n\\n1694\\n\\n1414\\n\\n510\\n\\n520\\n\\n673\\n\\n298\\n\\n10\\n\\n1\\n\\n2\\n\\n674\\n\\n111\\n\\n0\\n\\n40\\n\\n6\\n\\n28\\n\\n34\\n\\n263\\n\\nVeggie Salad\\n\\nSalad\\n\\nBurrito\\n\\nCrispy Tacos\\n\\nBowl\\n\\n6 Pack Soft Drink\\n\\nChips and Guacamole\\n\\nIzze\\n\\nNantucket Nectar\\n\\nChips and Mild Fresh Tomato Salsa\\n\\nChips and Tomatillo Red Chili Salsa\\n\\n233\\n\\nChips and Roasted Chili Corn Salsa\\n\\n38 Chips and Tomatillo Green Chili Salsa\\n\\n3 Chips and Tomatillo-Green Chili Salsa\\n\\n300\\n\\nChips and Tomatillo-Red Chili Salsa\\n\\n191\\n\\nChips and Roasted Chili-Corn Salsa\\n\\nChips and Fresh Tomato Salsa\\n\\nChips\\n\\nSide of Chips\\n\\nCanned Soft Drink\\n\\nCanned Soda\\n\\nBottled Water\\n\\n8.49\\n\\n7.40\\n\\n7.40\\n\\n7.40\\n\\n7.40\\n\\n6.49\\n\\n4.45\\n\\n3.39\\n\\n3.39\\n\\n3.00\\n\\n2.95\\n\\n2.95\\n\\n2.95\\n\\n2.39\\n\\n2.39\\n\\n2.39\\n\\n2.39\\n\\n2.15\\n\\n1.69\\n\\n1.25\\n\\n1.09\\n\\n1.09\\n\\nIn [33]:\\n\\n#What was the quantity of the most expensive item ordered?\\n\\nIn [34]:\\n\\ndf.sort_values(by = \"item_price\", ascending = False).head(1)\\n\\nOut[34]:\\n\\norder_id quantity\\n\\nitem_name choice_description item_price\\n\\n3598\\n\\n1443\\n\\n15 Chips and Fresh Tomato Salsa\\n\\nNaN\\n\\n44.25\\n\\nIn [35]:\\n\\n#How many times were a Veggie Salad Bowl ordered?\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n5/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [36]:\\n\\ndf[df.item_name == \"Veggie Salad Bowl\"]\\n \\n \\n\\nOut[36]:\\n\\norder_id quantity\\n\\nitem_name\\n\\nchoice_description item_price\\n\\n186\\n\\n83\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\\n\\n11.25\\n\\n295\\n\\n128\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\\n\\n11.25\\n\\n455\\n\\n195\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\\n\\n11.25\\n\\n496\\n\\n207\\n\\n[Fresh Tomato Salsa, [Rice, Lettuce,\\nGuacamole...\\n\\n11.25\\n\\n960\\n\\n394\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\\n\\n8.75\\n\\n1316\\n\\n536\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\\n\\n8.75\\n\\n1884\\n\\n760\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\\n\\n11.25\\n\\n2156\\n\\n869\\n\\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\\n\\n11.25\\n\\n2223\\n\\n896\\n\\n[Roasted Chili Corn Salsa, Fajita Vegetables]\\n\\n8.75\\n\\n2269\\n\\n913\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\\n\\n8.75\\n\\n2683\\n\\n1066\\n\\n[Roasted Chili Corn Salsa, [Fajita Vegetables,...\\n\\n8.75\\n\\n3223\\n\\n1289\\n\\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\\n\\n11.25\\n\\n3293\\n\\n1321\\n\\n[Fresh Tomato Salsa, [Rice, Black Beans,\\nChees...\\n\\n8.75\\n\\n4109\\n\\n1646\\n\\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\\n\\n11.25\\n\\n4201\\n\\n1677\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Black...\\n\\n11.25\\n\\n4261\\n\\n1700\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\\n\\n11.25\\n\\n4541\\n\\n1805\\n\\n[Tomatillo Green Chili Salsa, [Fajita Vegetabl...\\n\\n8.75\\n\\n4573\\n\\n1818\\n\\n[Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\\n\\n8.75\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\nVeggie Salad\\nBowl\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\nIn [15]:\\n\\nimport numpy as np\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n6/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [16]:\\n\\ndf = pd.read_csv(\\'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/\\ndf.head()\\n\\nOut[16]:\\n\\ncountry beer_servings spirit_servings wine_servings total_litres_of_pure_alcohol continent\\n\\n0 Afghanistan\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nAlbania\\n\\nAlgeria\\n\\nAndorra\\n\\nAngola\\n\\n0\\n\\n89\\n\\n25\\n\\n245\\n\\n217\\n\\n132\\n\\n0\\n\\n0\\n\\n138\\n\\n57\\n\\n0\\n\\n54\\n\\n14\\n\\n312\\n\\n45\\n\\n0.0\\n\\n4.9\\n\\n0.7\\n\\n12.4\\n\\n5.9\\n\\nAS\\n\\nEU\\n\\nAF\\n\\nEU\\n\\nAF\\n\\nIn [17]:\\n\\n#Which continent drinks more beer on average?\\n\\nIn [18]:\\n\\ndf.groupby(\\'continent\\').beer_servings.mean()\\n\\nOut[18]: continent \\n\\nAF     61.471698 \\nAS     37.045455 \\nEU    193.777778 \\nOC     89.687500 \\nSA    175.083333 \\nName: beer_servings, dtype: float64\\n\\nIn [19]:\\n\\n#For each continent print the statistics for wine consumption.\\n\\nIn [20]:\\n\\ndf.groupby(\\'continent\\').wine_servings.describe()\\n\\nOut[20]:\\n\\ncount\\n\\nmean\\n\\nstd min 25% 50%\\n\\n75% max\\n\\ncontinent\\n\\nAF\\n\\nAS\\n\\nEU\\n\\n53.0\\n\\n16.264151 38.846419\\n\\n0.0\\n\\n2.0\\n\\n13.00 233.0\\n\\n44.0\\n\\n9.068182 21.667034\\n\\n0.0\\n\\n1.0\\n\\n8.00 123.0\\n\\n45.0 142.222222 97.421738\\n\\n0.0\\n\\n59.0 128.0 195.00 370.0\\n\\nOC\\n\\n16.0\\n\\n35.625000 64.555790\\n\\n0.0\\n\\n8.5\\n\\n23.25 212.0\\n\\nSA\\n\\n12.0\\n\\n62.416667 88.620189\\n\\n1.0\\n\\n12.0\\n\\n98.50 221.0\\n\\n1.0\\n\\n0.0\\n\\n1.0\\n\\n3.0\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n7/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [21]:\\n\\nurl = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Ap\\ncrime = pd.read_csv(url)\\ncrime.head()\\n\\nOut[21]:\\n\\nYear Population\\n\\nTotal Violent Property Murder Forcible_Rape Robbery Aggravated_assa\\n\\n0 1960\\n\\n179323175 3384200 288460\\n\\n3095700\\n\\n9110\\n\\n17190\\n\\n107840\\n\\n1 1961\\n\\n182992000 3488000 289390\\n\\n3198600\\n\\n8740\\n\\n17220\\n\\n106670\\n\\n2 1962\\n\\n185771000 3752200 301510\\n\\n3450700\\n\\n8530\\n\\n17550\\n\\n110860\\n\\n3 1963\\n\\n188483000 4109500 316970\\n\\n3792500\\n\\n8640\\n\\n17650\\n\\n116470\\n\\n4 1964\\n\\n191141000 4564600 364220\\n\\n4200400\\n\\n9360\\n\\n21420\\n\\n130390\\n\\n1543\\n\\n1567\\n\\n1645\\n\\n1742\\n\\n2030\\n\\nIn [22]:\\n\\n#Convert the type of the column Year to datetime64\\n\\nIn [23]:\\n\\ncrime.Year = pd.to_datetime(crime.Year, format=\\'%Y\\')\\ncrime.info()\\n\\n<class \\'pandas.core.frame.DataFrame\\'> \\nRangeIndex: 55 entries, 0 to 54 \\nData columns (total 12 columns): \\nYear                  55 non-null datetime64[ns] \\nPopulation            55 non-null int64 \\nTotal                 55 non-null int64 \\nViolent               55 non-null int64 \\nProperty              55 non-null int64 \\nMurder                55 non-null int64 \\nForcible_Rape         55 non-null int64 \\nRobbery               55 non-null int64 \\nAggravated_assault    55 non-null int64 \\nBurglary              55 non-null int64 \\nLarceny_Theft         55 non-null int64 \\nVehicle_Theft         55 non-null int64 \\ndtypes: datetime64[ns](1), int64(11) \\nmemory usage: 5.2 KB \\n\\nIn [24]:\\n\\n#Set the Year column as the index of the dataframe\\xc2\\xb6\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n8/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [25]:\\n\\ncrime = crime.set_index(\\'Year\\', drop = True)\\ncrime.head()\\n\\nOut[25]:\\n\\nPopulation\\n\\nTotal Violent Property Murder Forcible_Rape Robbery Aggravated_assault\\n\\n179323175 3384200 288460\\n\\n3095700\\n\\n9110\\n\\n17190\\n\\n107840\\n\\n154320\\n\\n182992000 3488000 289390\\n\\n3198600\\n\\n8740\\n\\n17220\\n\\n106670\\n\\n156760\\n\\n185771000 3752200 301510\\n\\n3450700\\n\\n8530\\n\\n17550\\n\\n110860\\n\\n164570\\n\\n188483000 4109500 316970\\n\\n3792500\\n\\n8640\\n\\n17650\\n\\n116470\\n\\n174210\\n\\n191141000 4564600 364220\\n\\n4200400\\n\\n9360\\n\\n21420\\n\\n130390\\n\\n203050\\n\\nIn [26]:\\n\\n#Group the year by decades and sum the values\\xc2\\xb6\\n\\nIn [27]:\\n\\n# Uses resample to sum each decade\\ncrimes = crime.resample(\\'10AS\\').sum()\\n \\n# Uses resample to get the max value only for the \"Population\" column\\npopulation = crime[\\'Population\\'].resample(\\'10AS\\').max()\\n \\n# Updating the \"Population\" column\\ncrimes[\\'Population\\'] = population\\n \\ncrimes\\n\\nOut[27]:\\n\\nPopulation\\n\\nTotal\\n\\nViolent\\n\\nProperty\\n\\nMurder Forcible_Rape\\n\\nRobbery Agg\\n\\n201385000.0\\n\\n49295900.0\\n\\n4134930.0\\n\\n45160900.0 106180.0\\n\\n236720.0 1633510.0\\n\\n220099000.0 100991600.0\\n\\n9607930.0\\n\\n91383800.0 192230.0\\n\\n554570.0 4159020.0\\n\\n248239000.0\\n\\n131123369.0 14074328.0\\n\\n117048900.0 206439.0\\n\\n865639.0 5383109.0\\n\\n272690813.0 136582146.0 17527048.0\\n\\n119053499.0\\n\\n211664.0\\n\\n998827.0 5748930.0\\n\\n307006550.0\\n\\n115012044.0 13968056.0 100944369.0 163068.0\\n\\n922499.0 4230366.0\\n\\n318857056.0\\n\\n50167967.0\\n\\n6072017.0\\n\\n44095950.0\\n\\n72867.0\\n\\n421059.0 1749809.0\\n\\nNaN\\n\\nNaN\\n\\nNaN\\n\\nNaN\\n\\nNaN\\n\\nNaN\\n\\nNaN\\n\\nYear\\n\\n1960-\\n01-01\\n\\n1961-\\n01-01\\n\\n1962-\\n01-01\\n\\n1963-\\n01-01\\n\\n1964-\\n01-01\\n\\nYear\\n\\n1960-\\n01-01\\n\\n1970-\\n01-01\\n\\n1980-\\n01-01\\n\\n1990-\\n01-01\\n\\n2000-\\n01-01\\n\\n2010-\\n01-01\\n\\n2020-\\n01-01\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n9/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [28]:\\n\\ncrime.head()\\n\\nOut[28]:\\n\\nPopulation\\n\\nTotal Violent Property Murder Forcible_Rape Robbery Aggravated_assault\\n\\nYear\\n\\n1960-\\n01-01\\n\\n1961-\\n01-01\\n\\n1962-\\n01-01\\n\\n1963-\\n01-01\\n\\n1964-\\n01-01\\n\\nYear\\n\\n1960-\\n01-01\\n\\n1961-\\n01-01\\n\\n1962-\\n01-01\\n\\n179323175 3384200 288460\\n\\n3095700\\n\\n9110\\n\\n17190\\n\\n107840\\n\\n154320\\n\\n182992000 3488000 289390\\n\\n3198600\\n\\n8740\\n\\n17220\\n\\n106670\\n\\n156760\\n\\n185771000 3752200 301510\\n\\n3450700\\n\\n8530\\n\\n17550\\n\\n110860\\n\\n164570\\n\\n188483000 4109500 316970\\n\\n3792500\\n\\n8640\\n\\n17650\\n\\n116470\\n\\n174210\\n\\n191141000 4564600 364220\\n\\n4200400\\n\\n9360\\n\\n21420\\n\\n130390\\n\\n203050\\n\\nIn [31]:\\n\\n#Return the first 3 rows of the DataFrame df.\\n \\ndf = crime\\n\\nIn [32]:\\n\\ndf.iloc[:3]\\n\\nOut[32]:\\n\\nPopulation\\n\\nTotal Violent Property Murder Forcible_Rape Robbery Aggravated_assault\\n\\n179323175 3384200 288460\\n\\n3095700\\n\\n9110\\n\\n17190\\n\\n107840\\n\\n154320\\n\\n182992000 3488000 289390\\n\\n3198600\\n\\n8740\\n\\n17220\\n\\n106670\\n\\n156760\\n\\n185771000 3752200 301510\\n\\n3450700\\n\\n8530\\n\\n17550\\n\\n110860\\n\\n164570\\n\\nIn [33]:\\n\\n#Select just the \\'Murder\\' and \\'Robbery\\' columns from the DataFrame df and print f\\n\\nIn [35]:\\n\\ndf.loc[:, [\\'Murder\\', \\'Robbery\\']].head()\\n\\nOut[35]:\\n\\nMurder Robbery\\n\\nYear\\n\\n1960-01-01\\n\\n9110\\n\\n107840\\n\\n1961-01-01\\n\\n8740\\n\\n106670\\n\\n1962-01-01\\n\\n8530\\n\\n110860\\n\\n1963-01-01\\n\\n8640\\n\\n116470\\n\\n1964-01-01\\n\\n9360\\n\\n130390\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n10/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [37]:\\n\\ndf[[\\'Murder\\', \\'Robbery\\']].head()\\n\\nOut[37]:\\n\\nMurder Robbery\\n\\nYear\\n\\n1960-01-01\\n\\n9110\\n\\n107840\\n\\n1961-01-01\\n\\n8740\\n\\n106670\\n\\n1962-01-01\\n\\n8530\\n\\n110860\\n\\n1963-01-01\\n\\n8640\\n\\n116470\\n\\n1964-01-01\\n\\n9360\\n\\n130390\\n\\nYear\\n\\n1963-01-01\\n\\n8640\\n\\n116470\\n\\n1964-01-01\\n\\n9360\\n\\n130390\\n\\n1968-01-01\\n\\n13800\\n\\n262840\\n\\nIn [38]:\\n\\n#Select the data in rows [3, 4, 8] and in columns [\\'Murder\\', \\'Robbery\\']\\n\\nIn [39]:\\n\\ndf.loc[df.index[[3, 4, 8]], [\\'Murder\\', \\'Robbery\\']]\\n\\nOut[39]:\\n\\nMurder Robbery\\n\\nIn [45]:\\n\\n#Select only the rows where the number of murder is greater than 24,000\\n\\nIn [46]:\\n\\ndf[df[\\'Murder\\'] > 24000]\\n\\nOut[46]:\\n\\nPopulation\\n\\nTotal\\n\\nViolent\\n\\nProperty Murder Forcible_Rape Robbery Aggravated_assa\\n\\nYear\\n\\n1991-\\n01-01\\n\\n1993-\\n01-01\\n\\n252177000 14872900 1911770\\n\\n12961100\\n\\n24700\\n\\n106590\\n\\n687730\\n\\n257908000 14144800 1926020 12218800\\n\\n24530\\n\\n106010\\n\\n659870\\n\\n10927\\n\\n11356\\n\\nIn [47]:\\n\\n#Select the rows the murder is between 20k and 24k (inclusive)\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n11/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [51]:\\n\\ndf[df[\\'Murder\\'].between(20000, 24000)]\\n\\nOut[51]:\\n\\nPopulation\\n\\nTotal\\n\\nViolent\\n\\nProperty Murder Forcible_Rape Robbery Aggravated_assa\\n\\nYear\\n\\n1974-\\n01-01\\n\\n1975-\\n01-01\\n\\n1979-\\n01-01\\n\\n1980-\\n01-01\\n\\n1981-\\n01-01\\n\\n1982-\\n01-01\\n\\n1986-\\n01-01\\n\\n1987-\\n01-01\\n\\n1988-\\n01-01\\n\\n1989-\\n01-01\\n\\n1990-\\n01-01\\n\\n1992-\\n01-01\\n\\n1994-\\n01-01\\n\\n1995-\\n01-01\\n\\n211392000 10253400\\n\\n974720\\n\\n9278700\\n\\n20710\\n\\n55400\\n\\n442400\\n\\n213124000\\n\\n11292400 1039710 10252700\\n\\n20510\\n\\n56090\\n\\n470500\\n\\n220099000 12249500 1208030\\n\\n11041500\\n\\n21460\\n\\n76390\\n\\n480700\\n\\n225349264 13408300 1344520 12063700\\n\\n23040\\n\\n82990\\n\\n565840\\n\\n229146000 13423800 1361820 12061900\\n\\n22520\\n\\n82500\\n\\n592910\\n\\n231534000 12974400 1322390\\n\\n11652000\\n\\n21010\\n\\n78770\\n\\n553130\\n\\n240132887\\n\\n13211869 1489169\\n\\n11722700\\n\\n20613\\n\\n91459\\n\\n542775\\n\\n242282918 13508700 1483999 12024700\\n\\n20096\\n\\n91110\\n\\n517704\\n\\n245807000 13923100 1566220 12356900\\n\\n20680\\n\\n92490\\n\\n542970\\n\\n248239000 14251400 1646040 12605400\\n\\n21500\\n\\n94500\\n\\n578330\\n\\n248709873 14475600 1820130 12655500\\n\\n23440\\n\\n102560\\n\\n639270\\n\\n10548\\n\\n255082000 14438200 1932270 12505900\\n\\n23760\\n\\n109060\\n\\n672480\\n\\n260341000 13989500 1857670 12131900\\n\\n23330\\n\\n102220\\n\\n618950\\n\\n262755000 13862700 1798790 12063900\\n\\n21610\\n\\n97470\\n\\n580510\\n\\n10992\\n\\n4562\\n\\n4926\\n\\n6294\\n\\n6726\\n\\n6639\\n\\n6694\\n\\n8343\\n\\n8550\\n\\n9100\\n\\n9517\\n\\n11269\\n\\n11131\\n\\nIn [52]:\\n\\n#Calculate the mean murder for each different year in df.\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n12/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [53]:\\n\\ndf.groupby(\\'Year\\')[\\'Murder\\'].mean()\\n\\nOut[53]: Year \\n\\n1960-01-01     9110 \\n1961-01-01     8740 \\n1962-01-01     8530 \\n1963-01-01     8640 \\n1964-01-01     9360 \\n1965-01-01     9960 \\n1966-01-01    11040 \\n1967-01-01    12240 \\n1968-01-01    13800 \\n1969-01-01    14760 \\n1970-01-01    16000 \\n1971-01-01    17780 \\n1972-01-01    18670 \\n1973-01-01    19640 \\n1974-01-01    20710 \\n1975-01-01    20510 \\n1976-01-01    18780 \\n1977-01-01    19120 \\n1978-01-01    19560 \\n1979-01-01    21460 \\n1980-01-01    23040 \\n1981-01-01    22520 \\n1982-01-01    21010 \\n1983-01-01    19310 \\n1984-01-01    18690 \\n1985-01-01    18980 \\n1986-01-01    20613 \\n1987-01-01    20096 \\n1988-01-01    20680 \\n1989-01-01    21500 \\n1990-01-01    23440 \\n1991-01-01    24700 \\n1992-01-01    23760 \\n1993-01-01    24530 \\n1994-01-01    23330 \\n1995-01-01    21610 \\n1996-01-01    19650 \\n1997-01-01    18208 \\n1998-01-01    16914 \\n1999-01-01    15522 \\n2000-01-01    15586 \\n2001-01-01    16037 \\n2002-01-01    16229 \\n2003-01-01    16528 \\n2004-01-01    16148 \\n2005-01-01    16740 \\n2006-01-01    17030 \\n2007-01-01    16929 \\n2008-01-01    16442 \\n2009-01-01    15399 \\n2010-01-01    14772 \\n2011-01-01    14661 \\n2012-01-01    14866 \\n2013-01-01    14319 \\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n13/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\n2014-01-01    14249 \\nName: Murder, dtype: int64\\n\\nIn [55]:\\n\\n#Sort df first by the values in the \\'Murder\\' in decending order, \\n#then by the value in the \\'Violent\\' column in ascending order.\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n14/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [58]:\\n\\ndf.sort_values(by=[\\'Murder\\', \\'Violent\\'], ascending=[False, True])\\n\\nOut[58]:\\n\\nPopulation\\n\\nTotal\\n\\nViolent\\n\\nProperty Murder Forcible_Rape Robbery Aggravated_assa\\n\\n252177000 14872900 1911770\\n\\n12961100\\n\\n24700\\n\\n106590\\n\\n687730\\n\\n10927\\n\\n262755000 13862700 1798790 12063900\\n\\n21610\\n\\n97470\\n\\n580510\\n\\n10992\\n\\nYear\\n\\n1991-\\n01-01\\n\\n1993-\\n01-01\\n\\n1992-\\n01-01\\n\\n1990-\\n01-01\\n\\n1994-\\n01-01\\n\\n1980-\\n01-01\\n\\n1981-\\n01-01\\n\\n1995-\\n01-01\\n\\n1989-\\n01-01\\n\\n1979-\\n01-01\\n\\n1982-\\n01-01\\n\\n1974-\\n01-01\\n\\n1988-\\n01-01\\n\\n1986-\\n01-01\\n\\n1975-\\n01-01\\n\\n1987-\\n01-01\\n\\n1996-\\n01-01\\n\\n1973-\\n01-01\\n\\n1978-\\n01-01\\n\\n1983-\\n01-01\\n\\n1977-\\n01-01\\n\\n1985-\\n01-01\\n\\n257908000 14144800 1926020 12218800\\n\\n24530\\n\\n106010\\n\\n659870\\n\\n255082000 14438200 1932270 12505900\\n\\n23760\\n\\n109060\\n\\n672480\\n\\n248709873 14475600 1820130 12655500\\n\\n23440\\n\\n102560\\n\\n639270\\n\\n260341000 13989500 1857670 12131900\\n\\n23330\\n\\n102220\\n\\n618950\\n\\n225349264 13408300 1344520 12063700\\n\\n23040\\n\\n82990\\n\\n565840\\n\\n229146000 13423800 1361820 12061900\\n\\n22520\\n\\n82500\\n\\n592910\\n\\n248239000 14251400 1646040 12605400\\n\\n21500\\n\\n94500\\n\\n578330\\n\\n220099000 12249500 1208030\\n\\n11041500\\n\\n21460\\n\\n76390\\n\\n480700\\n\\n231534000 12974400 1322390\\n\\n11652000\\n\\n21010\\n\\n78770\\n\\n553130\\n\\n211392000 10253400\\n\\n974720\\n\\n9278700\\n\\n20710\\n\\n55400\\n\\n442400\\n\\n245807000 13923100 1566220 12356900\\n\\n20680\\n\\n92490\\n\\n542970\\n\\n240132887\\n\\n13211869 1489169\\n\\n11722700\\n\\n20613\\n\\n91459\\n\\n542775\\n\\n213124000\\n\\n11292400 1039710 10252700\\n\\n20510\\n\\n56090\\n\\n470500\\n\\n242282918 13508700 1483999 12024700\\n\\n20096\\n\\n91110\\n\\n517704\\n\\n209851000\\n\\n8718100\\n\\n875910\\n\\n7842200\\n\\n19640\\n\\n51400\\n\\n384220\\n\\n218059000\\n\\n11209000 1085550 10123400\\n\\n19560\\n\\n67610\\n\\n426930\\n\\n233981000 12108600 1258090 10850500\\n\\n19310\\n\\n78920\\n\\n506570\\n\\n216332000 10984500 1029580\\n\\n9955000\\n\\n19120\\n\\n63500\\n\\n412610\\n\\n238740000 12431400 1328800\\n\\n11102600\\n\\n18980\\n\\n88670\\n\\n497870\\n\\n11356\\n\\n11269\\n\\n10548\\n\\n11131\\n\\n6726\\n\\n6639\\n\\n9517\\n\\n6294\\n\\n6694\\n\\n4562\\n\\n9100\\n\\n8343\\n\\n4926\\n\\n8550\\n\\n4206\\n\\n5714\\n\\n6532\\n\\n5343\\n\\n7232\\n\\n15/18\\n\\n265228572 13493863 1688540\\n\\n11805300\\n\\n19650\\n\\n96250\\n\\n535590\\n\\n10370\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nPopulation\\n\\nTotal\\n\\nViolent\\n\\nProperty Murder Forcible_Rape Robbery Aggravated_assa\\n\\n214659000\\n\\n11349700 1004210 10345500\\n\\n18780\\n\\n57080\\n\\n427810\\n\\n236158000\\n\\n11881800 1273280 10608500\\n\\n18690\\n\\n84230\\n\\n485010\\n\\n208230000\\n\\n8248800\\n\\n834900\\n\\n7413900\\n\\n18670\\n\\n46850\\n\\n376290\\n\\n267637000 13194571 1634770\\n\\n11558175\\n\\n18208\\n\\n96153\\n\\n498534\\n\\n10232\\n\\nYear\\n\\n1976-\\n01-01\\n\\n1984-\\n01-01\\n\\n1972-\\n01-01\\n\\n1997-\\n01-01\\n\\n1971-\\n01-01\\n\\n2006-\\n01-01\\n\\n2007-\\n01-01\\n\\n1998-\\n01-01\\n\\n2005-\\n01-01\\n\\n2003-\\n01-01\\n\\n2008-\\n01-01\\n\\n2002-\\n01-01\\n\\n2004-\\n01-01\\n\\n2001-\\n01-01\\n\\n1970-\\n01-01\\n\\n2000-\\n01-01\\n\\n1999-\\n01-01\\n\\n2009-\\n01-01\\n\\n2012-\\n01-01\\n\\n2010-\\n01-01\\n\\n1969-\\n01-01\\n\\n2011-\\n01-01\\n\\n2013-\\n01-01\\n\\n206212000\\n\\n8588200\\n\\n816500\\n\\n7771700\\n\\n17780\\n\\n42260\\n\\n387700\\n\\n299398484\\n\\n11401511 1418043\\n\\n9983568\\n\\n17030\\n\\n92757\\n\\n447403\\n\\n301621157\\n\\n11251828 1408337\\n\\n9843481\\n\\n16929\\n\\n90427\\n\\n445125\\n\\n270296000 12475634 1531044 10944590\\n\\n16914\\n\\n93103\\n\\n446625\\n\\n296507061\\n\\n11565499 1390745 10174754\\n\\n16740\\n\\n94347\\n\\n417438\\n\\n290690788\\n\\n11826538 1383676 10442862\\n\\n16528\\n\\n93883\\n\\n414235\\n\\n304374846\\n\\n11160543 1392628\\n\\n9767915\\n\\n16442\\n\\n90479\\n\\n443574\\n\\n287973924\\n\\n11878954 1423677 10455277\\n\\n16229\\n\\n95235\\n\\n420806\\n\\n293656842\\n\\n11679474 1360088 10319386\\n\\n16148\\n\\n95089\\n\\n401470\\n\\n285317559\\n\\n11876669 1439480 10437480\\n\\n16037\\n\\n90863\\n\\n423557\\n\\n203235298\\n\\n8098000\\n\\n738820\\n\\n7359200\\n\\n16000\\n\\n37990\\n\\n349860\\n\\n281421906\\n\\n11608072 1425486 10182586\\n\\n15586\\n\\n90178\\n\\n408016\\n\\n272690813\\n\\n11634378 1426044 10208334\\n\\n15522\\n\\n89411\\n\\n409371\\n\\n307006550 10762956 1325896\\n\\n9337060\\n\\n15399\\n\\n89241\\n\\n408742\\n\\n313873685 10219059 1217067\\n\\n9001992\\n\\n14866\\n\\n85141\\n\\n355051\\n\\n309330219 10363873 1251248\\n\\n9112625\\n\\n14772\\n\\n85593\\n\\n369089\\n\\n201385000\\n\\n7410900\\n\\n661870\\n\\n6749000\\n\\n14760\\n\\n37170\\n\\n298850\\n\\n311587816 10258774 1206031\\n\\n9052743\\n\\n14661\\n\\n84175\\n\\n354772\\n\\n316497531\\n\\n9850445 1199684\\n\\n8650761\\n\\n14319\\n\\n82109\\n\\n345095\\n\\n5005\\n\\n6853\\n\\n3930\\n\\n3687\\n\\n8608\\n\\n8558\\n\\n9744\\n\\n8622\\n\\n8590\\n\\n8421\\n\\n8914\\n\\n8473\\n\\n9090\\n\\n3349\\n\\n9117\\n\\n9117\\n\\n8125\\n\\n7620\\n\\n7818\\n\\n3110\\n\\n7524\\n\\n7265\\n\\n16/18\\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nPopulation\\n\\nTotal\\n\\nViolent\\n\\nProperty Murder Forcible_Rape Robbery Aggravated_assa\\n\\nYear\\n\\n2014-\\n01-01\\n\\n1968-\\n01-01\\n\\n1967-\\n01-01\\n\\n1966-\\n01-01\\n\\n1965-\\n01-01\\n\\n1964-\\n01-01\\n\\n1960-\\n01-01\\n\\n1961-\\n01-01\\n\\n1963-\\n01-01\\n\\n1962-\\n01-01\\n\\n318857056\\n\\n9475816 1197987\\n\\n8277829\\n\\n14249\\n\\n84041\\n\\n325802\\n\\n199399000\\n\\n6720200\\n\\n595010\\n\\n6125200\\n\\n13800\\n\\n31670\\n\\n262840\\n\\n197457000\\n\\n5903400\\n\\n499930\\n\\n5403500\\n\\n12240\\n\\n27620\\n\\n202910\\n\\n195576000\\n\\n5223500\\n\\n430180\\n\\n4793300\\n\\n11040\\n\\n25820\\n\\n157990\\n\\n193526000\\n\\n4739400\\n\\n387390\\n\\n4352000\\n\\n9960\\n\\n23410\\n\\n138690\\n\\n191141000\\n\\n4564600\\n\\n364220\\n\\n4200400\\n\\n9360\\n\\n21420\\n\\n130390\\n\\n179323175\\n\\n3384200\\n\\n288460\\n\\n3095700\\n\\n9110\\n\\n17190\\n\\n107840\\n\\n182992000\\n\\n3488000\\n\\n289390\\n\\n3198600\\n\\n8740\\n\\n17220\\n\\n106670\\n\\n188483000\\n\\n4109500\\n\\n316970\\n\\n3792500\\n\\n8640\\n\\n17650\\n\\n116470\\n\\n185771000\\n\\n3752200\\n\\n301510\\n\\n3450700\\n\\n8530\\n\\n17550\\n\\n110860\\n\\n7412\\n\\n2867\\n\\n2571\\n\\n2353\\n\\n2153\\n\\n2030\\n\\n1543\\n\\n1567\\n\\n1742\\n\\n1645\\n\\nIn [59]:\\n\\n#read the following data set\\n#https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datase\\n\\nIn [60]:\\n\\ndf = pd.read_csv(\\'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/\\n\\nIn [61]:\\n\\ndf.head()\\n\\nOut[61]:\\n\\nUnnamed: 0 mpg cyl\\n\\ndisp\\n\\nhp drat\\n\\nwt\\n\\nqsec vs am gear carb\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\nMazda RX4\\n\\n21.0\\n\\n6 160.0\\n\\n110 3.90 2.620 16.46\\n\\nMazda RX4 Wag\\n\\n21.0\\n\\n6 160.0\\n\\n110 3.90 2.875 17.02\\n\\nDatsun 710\\n\\n22.8\\n\\n4 108.0\\n\\n93 3.85 2.320 18.61\\n\\nHornet 4 Drive\\n\\n21.4\\n\\n6 258.0\\n\\n110 3.08 3.215 19.44\\n\\n4 Hornet Sportabout\\n\\n18.7\\n\\n8 360.0 175 3.15 3.440 17.02\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n3\\n\\n4\\n\\n4\\n\\n1\\n\\n1\\n\\n2\\n\\nIn [62]:\\n\\n#For each cyl type and each number of gears, find the mean mileage. \\n \\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n17/18\\n\\n\\x0c2/28/2019\\n\\nLet\\'s Do Together_pandas\\n\\nIn [65]:\\n\\ndf.pivot_table(index=\\'cyl\\', columns=\\'gear\\', values=\\'mpg\\', aggfunc=\\'mean\\')\\n \\n\\nOut[65]:\\n\\ngear\\n\\n3\\n\\n4\\n\\n5\\n\\ncyl\\n\\n4 21.50 26.925 28.2\\n\\n6 19.75 19.750 19.7\\n\\n8 15.05\\n\\nNaN 15.4\\n\\nIn [ ]:\\n\\n \\n\\nhttp://localhost:8888/notebooks/Let\\'s%20Do%20Together_pandas.ipynb\\n\\n18/18\\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b\"This assignment will help you to consolidate the concepts learnt in the session. \\n\\n \\n\\n \\n\\n\\xe2\\x80\\x8bIntroduction \\n1.\\n\\xe2\\x80\\x8b  \\n\\n \\n\\n2.\\n\\xe2\\x80\\x8bProblem Statement \\n\\xe2\\x80\\x8b  \\n  \\n\\nimport pandas as pd \\n\\nimport numpy as np \\n\\nimport matplotlib.pyplot as plt \\n\\n%matplotlib inline \\n\\ndf\\n\\n.head(2) \\n\\ndf1\\n \\npd.read_csv('\\nin_weather_oldest.csv\\n\\n') \\n\\ndf1\\n\\n.head(2) \\n\\n1. Get the Metadata from the above files. \\n\\nExpected Output: \\n\\n \\ndf\\npd.read_csv('\\n/data-text.csv\\n\\n \\n=\\nhttps://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3\\n') \\n\\n \\n=\\nhttps://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berl\\n\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\x0c \\n\\n \\n\\n \\n\\n\\x0c2. Get the row names from the above files. \\n\\nExpected Output: \\n\\n \\n\\n3. Change the column name from any of the above file. \\n\\nExpected Output: \\n\\n \\n\\n4. Change the column name from any of the above file and store the changes made\\n \\n \\npermanently. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nExpected Output: \\n\\n5.  Change the names of multiple columns. \\n\\nExpected Output: \\n\\n6.  Arrange values of a particular column in ascending order. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0cExpected Output: \\n\\n7.  Arrange multiple column values in ascending order. \\n\\nExpected Output: \\n\\n8.  Make \\n\\ncountry\\n\\n as the first column of the dataframe. \\n\\nExpected Output: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\x0c9.  Get the column array using a variable \\n\\nExpected Output: \\n\\n10. Get the subset rows 11, 24, 37 \\n\\nExpected Output: \\n\\n \\n\\nExpected Output: \\n\\n11. Get the subset rows excluding 5, 12, 23, and 56 \\n\\n \\n\\n \\n\\n \\n\\nLoad datasets from CSV \\n\\n \\nusers\\npd.read_csv('\\n') \\nv\\n\\n \\n=\\nhttps://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.cs\\n\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\x0c \\n=\\nhttps://raw.githubusercontent.com/ben519/DataWrangling/master/Data/session\\n\\n \\n=\\nhttps://raw.githubusercontent.com/ben519/DataWrangling/master/Data/product\\n\\n \\n=\\nhttps://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transac\\n\\n \\n\\n12. Join users to transactions, keeping all rows from transactions and only matching rows from\\n \\n \\n \\nusers (left join) \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\nsessions\\npd.read_csv('\\n') \\ns.csv\\n\\nproducts\\n \\npd.read_csv('\\n') \\ns.csv\\n\\ntransactions\\n \\npd.read_csv('\\ntions.csv\\n\\n') \\n\\nusers.head() \\n\\nsessions.head() \\n\\ntransactions.head() \\n\\nExpected Output: \\n\\n \\n\\n \\n\\n \\n\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\x0c13.  Which transactions have a UserID not in users? \\n\\nExpected Output: \\n\\n \\nJoin users to transactions, keeping only rows from transactions and users that match via\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n14.\\n \\n \\n \\nUserID (inner join) \\n\\n \\n\\nExpected Output: \\n\\n \\nJoin users to transactions, displaying all matching rows AND all non-matching rows (full\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n15.\\n \\nouter join) \\n\\nExpected Output: \\n\\n\\x0c16.  Determine which sessions occurred on the same day each user registered \\n\\nExpected Output: \\n\\nExpected Output: \\n\\n17.  Build a dataset with every possible (UserID, ProductID) pair (cross join) \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nExpected Output: \\n\\n18.  Determine how much quantity of each product was purchased by each user \\n\\n\\x0c \\n\\n \\n\\nExpected Output: \\n\\n19. For each user, get each possible pair  of pair transactions (TransactionID1, TransacationID2) \\n\\n\\x0c20. Join each user to his/her first occuring transaction in the transactions table \\n\\nExpected Output: \\n\\n \\n\\n \\n\\n21. Test to see if we can drop columns \\n\\nCode with Output : \\n\\nmy_columns = list(data.columns) \\n\\nmy_columns \\n\\n['UserID', \\n 'User', \\n 'Gender', \\n 'Registered', \\n\\n\\x0c 'Cancelled', \\n 'TransactionID', \\n 'TransactionDate', \\n 'ProductID', \\n 'Quantity'] \\n\\nlist(data.dropna(thresh=int(data.shape[0] * .9), axis=1).columns) #set threshold to drop NAs \\n\\n['UserID', 'User', 'Gender', 'Registered'] \\n\\nmissing_info = list(data.columns[data.isnull().any()]) \\n\\n['Cancelled', 'TransactionID', 'TransactionDate', 'ProductID', 'Quantity'] \\n\\nmissing_info \\n\\n//for col in missing_info: \\n\\n  \\n\\n  num_missing = data[data[col].isnull() == True].shape[0] \\n\\n    print('number missing for column {}: {}'.format(col, num_missing)) \\n\\nOutput: Count of missing data \\n\\nnumber missing for column Cancelled: 3 \\nnumber missing for column TransactionID: 2 \\nnumber missing for column TransactionDate: 2 \\nnumber missing for column ProductID: 2 \\nnumber missing for column Quantity: 2 \\n\\n \\n\\n//\\n\\nfor col in missing_info: \\n\\n  num_missing = data[data[col].isnull() == True].shape[0] \\n\\n   print('number missing for column {}: {}'.format(col, num_missing)) #count of missing data \\n\\n   for col in missing_info: \\n\\n    percent_missing = data[data[col].isnull() == True].shape[0] / data.shape[0] \\n\\n    print('percent missing for column {}: {}'.format( \\n\\n    col, percent_missing)) \\n\\n  Output of percentage missing data \\n\\n\\xe2\\x80\\x8b\\n\\xe2\\x80\\x8b\\n\\x0c\"-------------this is the end of file----------------------------\n",
      "b'1. What are the di\\xef\\xac\\x80erences between operators and values in the following? \\n\\nSol: * - / + are operators and \"hello\" -87.8 6 are values. \\n\\n2. What is the di\\xef\\xac\\x80erence between string and variable? \\n\\nsol: string starts and ends with single or double quotes \\n\\n3. Describe three di\\xef\\xac\\x80erent data forms. \\n\\nsol: strings,integer and \\xef\\xac\\x82oa\\x00ng point \\n\\n4. What makes up an expression? What are the func\\x00ons of all expressions? \\n\\nsol: expression is combina\\x00on of opearators and value. Expression is solved down to a single \\nvalue \\n\\n5. In this chapter, assignment statements such as spam = 10 were added. What\\'s the \\ndi\\xef\\xac\\x80erence between a declara\\x00on and an expression? \\n\\nSOL: Expression is solved down to \\xef\\xac\\x81nd a value and declara\\x00on is assigning a value directly. \\n\\n6. A\\x00er running the following code, what does the variable bacon contain? \\n\\nsol: bacon value will not change as bacon +1 is not reassigned or equated to bacon variable. \\n\\n\\'hello\\' \\n\\n-87.8  \\n\\nspam \\n\\n\\'spam\\' \\n\\n \\n\\n* \\n\\n- \\n\\n/ \\n\\n+ \\n\\n6 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nbacon = 22 \\n\\nbacon + 1 \\n\\n\\x0c7. What should the values of the following two terms be? \\n\\n\\'spam\\' + \\'spamspam\\' \\n\\n\\'spam\\' * 3 \\n\\nsol: \"both will be \\'spamspamspam\\' \\n\\n8. Why is it that eggs is a true variable name but 100 is not? \\n\\nsol. variable name cannot start with a number but it can end with number ex: a100=5. this is \\nvalid. but 100a=5 is not valid. \\n\\n9. Which of the following three func\\x00ons may be used to convert a value to an integer, a \\n\\xef\\xac\\x82oa\\x00ng-point number, or a string? \\n\\nsol:int() \\xef\\xac\\x82oat() str() can be used to convert to required data type \\n\\n10. What is the error caused by this expression? What would you do about it? \\n\\n\\'I have eaten \\' + 99 + \\' burritos.\\' \\n\\nsol:concatena\\x00on error as you cannot add integer to string values. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b'1. What are the Boolean data type\\'s two values? How do you go about wri\\x00ng them? \\n\\nsol: True and False \\n\\n2. What are the three di\\xef\\xac\\x80erent types of Boolean operators? \\n\\nsol. and or not \\n\\n3. Make a list of each Boolean operator\\'s truth tables (i.e. every possible combina\\x00on of Boolean \\nvalues for the operator and what it evaluate ). \\n\\nsol: \\n\\n \\nTrue and True\\xc2\\xa0is\\xc2\\xa0True.\\n\\n \\nTrue and False\\xc2\\xa0is\\xc2\\xa0False.\\n\\n \\nFalse and True\\xc2\\xa0is\\xc2\\xa0False.\\n\\n \\nFalse and False\\xc2\\xa0is\\xc2\\xa0False.\\n\\n \\nTrue or True\\xc2\\xa0is\\xc2\\xa0True.\\n\\n \\nTrue or False\\xc2\\xa0is\\xc2\\xa0True.\\n\\n \\nFalse or True\\xc2\\xa0is\\xc2\\xa0True.\\n\\n \\nFalse or False\\xc2\\xa0is\\xc2\\xa0False.\\n\\n \\nnot True\\xc2\\xa0is\\xc2\\xa0False.\\n\\n \\nnot\\xc2\\xa0False\\xc2\\xa0is\\xc2\\xa0True\\n\\n4. What are the values of the following expressions? \\n\\n(5 > 4) and (3 == 5)    False \\n\\nnot (5 > 4)      False \\n\\n(5 > 4) or (3 == 5)      True \\n\\nnot ((5 > 4) or (3 == 5))       False \\n\\n(True and True) and (True == False)      False \\n\\n(not False) or (not True)    True \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c5. What are the six di\\xef\\xac\\x80erent types of reference operators? \\n\\nsol:   \\n\\n==,\\xc2\\xa0!=,\\xc2\\xa0<,\\xc2\\xa0>,\\xc2\\xa0<=,\\xc2\\xa0and\\xc2\\xa0>=\\n \\n\\n6. How do you tell the di\\xef\\xac\\x80erence between the equal to and assignment operators? \\n\\nsol:  assignmet operator \"=\" stores a value in a variable whereas == equal to operator compares two \\nvalues and gives boolean output is true or false. \\n\\n7. Describe a condi\\x00on and when you would use one. \\n\\nsol: condi\\x00on is a statement that gives us boolean value and used in \\xef\\xac\\x82ow control statement. \\n\\n8. Recognize the following three blocks in this code: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nspam = 0 \\n\\nif spam == 10: \\n\\n    print(\\'eggs\\') \\n\\n    if spam > 5: \\n\\n        print(\\'bacon\\') \\n\\n    else: \\n\\n        print(\\'ham\\') \\n\\n    print(\\'spam\\') \\n\\nprint(\\'spam\\') \\n\\n \\n\\nsol: \\n\\n \\nprint(\\'eggs\\')\\n \\n\\n \\n  if spam > 5:\\n\\n \\n        print(\\'bacon\\')\\n\\n \\n  else:\\n\\n \\n       print(\\'ham\\')\\n\\n \\n  print(\\'spam\\')\\n\\n\\x0c9. Create a programme that prints. If 1 is stored in spam, prints Hello; if 2 is stored in spam, prints \\nHowdy; and if 3 is stored in spam, prints Saluta\\x00ons! if there\\'s something else in spam. \\n\\n10.If your programme is stuck in an endless loop, what keys can you press? \\n\\nsol; control+C \\n\\n11. How can you tell the di\\xef\\xac\\x80erence between break and con\\x00nue? \\n\\nSOL: Break statement breaks the loop and comes out of loop. \\n\\ncon\\x00nue statement send the control back to start of loop. \\n\\n12. In a for loop, what is the di\\xef\\xac\\x80erence between range(10), range(0, 10), and range(0, 10, 1)? \\n\\nsol: all three are same \\n\\n13. Using a for loop, write a short programme that prints the numbers 1 to 10 Then, using a while \\nloop, create an iden\\x00cal programme that prints the numbers 1 to 10. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nsol: \\n\\n \\nspam=int(input())\\n\\n \\nif spam == 1:\\n\\n \\n    print(\\'Hello\\')\\n\\n \\nelif spam == 2:\\n\\n \\n    print(\\'Howdy\\')\\n\\n \\nelse:\\n\\n \\n    print(\\'Gree\\x00ngs!\\')\\n\\nsol: \\n\\n for i in range(1,11): \\n\\nprint(i) \\n\\n a=1 \\n\\nwhile (a<11): \\n\\n\\x0c14. If you had a bacon() func\\x00on within a spam module, what would you call it a\\x00er impor\\x00ng \\nspam? \\n\\nprint(a) \\n\\na=a+1 \\n\\nsol: import spam \\n\\nspam.bacon() \\n\\n \\n\\n \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n                                                 \\n\\n                                                                   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nReview scraper from \\nscratch till deployment \\n\\nvirat Sagar \\n\\n \\n\\n[Date] \\n\\n[Course title] \\n\\n\\x0c \\n\\nTable of Contents \\nPreface ..................................................................................................................................................... 2 \\n\\nIntroduction............................................................................................................................................. 3 \\n\\nPrerequisites............................................................................................................................................ 5 \\n\\nPyCharm Installation .................................................................................................................. 6 \\n\\nMongoDB Installation ................................................................................................................ 8 \\n\\nStarting MongoDB  ................................................................................................................... 11 \\n\\nApplication Architecture ....................................................................................................................... 13 \\n\\nPython Implementation ........................................................................................................................ 13 \\n\\nHeroku ................................................................................................................................................... 19 \\n\\nHeroku Basics ........................................................................................................................................ 20 \\n\\nSteps before cloud deployment ........................................................................................................... 21 \\n\\nHeroku app creation and deployment ................................................................................................. 23 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nPreface \\n\\nThis book is intended towards helping all the data scientists out there. It is a step by \\nstep guide for creating a web scraper, in this case, a review scrapper right from scratch \\nand then deploying it to the heroku cloud platform. Text scrappers are extensively used \\nin the industry today for competitive pricing, market studies, customer sentiment \\nanalysis, etc. This book takes a simple example of an online cell phone purchase and \\ntries to explain the concepts simply, extensively, and thoroughly to create a review \\nscrapper right from scratch and then its deployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n1. Introduction: \\n\\nWeb Scraping(Text) \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xe2\\x80\\xa2  Web Crawling\\xe2\\x86\\x92 Accessing the webpages over the internet and pulling data from \\n\\nthem. \\n\\n\\xe2\\x80\\xa2  HTML Parsing\\xe2\\x86\\x92 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we buy a phone online? \\n\\n1.  We first look for a phone with good reviews \\n2.  We see on which website it\\xe2\\x80\\x99s available at the lowest price \\n3.  We check whether it\\xe2\\x80\\x99s  delivered in our area or not \\n4.  If everything looks good, then we buy the phone. \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nOther examples of the applications of web scrapping are: \\n\\n\\xe2\\x80\\xa2  Competitive pricing. \\n\\xe2\\x80\\xa2  Manufacturers monitor the market, whether the retailer is maintaining a minimum price \\n\\n\\xe2\\x80\\xa2  Sentiment analysis of the consumers, whether they are happy with the services and \\n\\nor not. \\n\\nproducts or not. \\n\\n\\xe2\\x80\\xa2  To aggregate news articles. \\n\\xe2\\x80\\xa2  To aggregate Marketing data. \\n\\xe2\\x80\\xa2  To gain financial insights from the market. \\n\\xe2\\x80\\xa2  To gather data for research. \\n\\xe2\\x80\\xa2  To generate marketing leads. \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  To collect trending topics by media houses. \\n\\nAnd, the list goes on.  \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of buying a phone online further and try to scrap the \\nreviews from the website about the phone that we are planning to buy. \\n\\nFor example, if we open filpkart.com and search for \\xe2\\x80\\x98iPhone\\xe2\\x80\\x99, the search result will be as \\nfollows: \\n \\n\\nThen if we click on a product link, it will take us to to the following page: \\n\\n \\n\\n \\n\\n4 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\nIf we scroll down on this page, we\\xe2\\x80\\x99ll get to see the comments posted by the customers: \\n\\n \\n\\nOur end goal is to build a web scraper that collects the reviews of a product from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xe2\\x80\\xa2  Python installed. \\n\\xe2\\x80\\xa2  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xe2\\x80\\xa2  Flask Installed. (A simple command: pip install flask) \\n\\xe2\\x80\\xa2  MongoDB installed (Explained Later). \\n\\xe2\\x80\\xa2  Basic understanding of Python and HTML. \\n\\xe2\\x80\\xa2  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n2.1  PyCharm Installation: \\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n12 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n              \\n \\n\\n \\n\\n \\n\\n4. Python Implementation: \\n\\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 on our local machines. \\n\\n \\n\\n13 | P a g e  \\n\\n\\x0c \\n\\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\xe2\\x80\\x98static\\xe2\\x80\\x99 and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 to hold \\nthe code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98css\\xe2\\x80\\x99 \\nfor keeping the stylesheets for our UI. \\n\\n3.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98flask_app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder.  \\n4.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the files: \\xe2\\x80\\x98main.css\\xe2\\x80\\x99 and \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The files are attached \\n\\nhere for reference.  \\n\\n5.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x98base.html\\xe2\\x80\\x99,\\xe2\\x80\\x99index.html\\xe2\\x80\\x99, \\n\\nand \\xe2\\x80\\x98results.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xe2\\x80\\xa2  base.html\\xe2\\x86\\x92 It acts as the common building block for the other two HTML \\n\\n \\n\\npages. \\n\\n\\xe2\\x80\\xa2  index.html\\xe2\\x86\\x92  Home page of our application. \\n\\xe2\\x80\\xa2  results.html\\xe2\\x86\\x92 Page to show the reviews for the searched keyword. \\n\\n6.   Now, the folder structure should look like: \\n\\n \\n\\n7.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\n \\n\\n14 | P a g e  \\n\\nbutton. \\n\\n \\n\\nbase.htmlindex.htmlresults.html\\x0cc)  The application now searches for reviews and shows the result on the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 \\n\\n \\n\\npage. \\n\\n8.  Understanding flask_app.py. \\n\\n \\na)  Import the necessary libraries: \\n\\nfrom flask import Flask, render_template, request,jsonify \\nfrom flask_cors import CORS,cross_origin \\nimport requests \\nfrom bs4 import BeautifulSoup as bs \\nfrom urllib.request import urlopen as uReq \\nimport pymongo \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\n\\nroute path, the control gets transferred inside the application. \\n\\n@app.route(\\'/\\',methods=[\\'POST\\',\\'GET\\']) # route with allowed \\nmethods as POST and GET \\n\\nd)  Now let\\xe2\\x80\\x99s understand the \\xe2\\x80\\x98index()\\xe2\\x80\\x99 function. \\n\\ni. \\n\\nIf the HTTP request method is POST(which is defined in index.html at \\nform submit action), then first check if the records for the searched \\nkeyword is already present in the database or not. If present, show that to \\nthe user. \\n\\nii. \\n\\ndbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \\n# opening a connection to Mongo \\ndb = dbConn[\\'crawlerDB\\'] # connecting to the database \\ncalled crawlerDB \\nreviews = db[searchString].find({}) # searching the \\ncollection with the name same as the keyword \\nif reviews.count() > 0: # if there is a collection with \\nsearched keyword and it has records in it \\n    return render_template(\\'results.html\\',reviews=reviews) \\n# show the results to user \\nIf the searched keyword doesn\\xe2\\x80\\x99t have a database entry, then the application \\ntries to fetch the details from the internet, as shown below: \\n\\nflipkart_url = \"https://www.flipkart.com/search?q=\" + \\nsearchString # preparing the URL to search the product on \\nFlipkart \\nuClient = uReq(flipkart_url) # requesting the webpage from \\nthe internet \\nflipkartPage = uClient.read() # reading the webpage \\nuClient.close() # closing the connection to the web server \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c \\n\\niii. \\n\\nflipkart_html = bs(flipkartPage, \"html.parser\") # parsing \\nthe webpage as HTML \\nOnce we have the entire HTML page, we try to get the product URL and \\nthen jump to the product page. It is similar to redirecting to the following \\npage: \\n\\nThe equivalent Python code is: \\n\\n \\n\\nproductLink = \"https://www.flipkart.com\" + \\nbox.div.div.div.a[\\'href\\'] # extracting the actual product \\nlink \\nprodRes = requests.get(productLink) # getting the product \\npage from server \\nprod_html = bs(prodRes.text, \"html.parser\") # parsing the \\nproduct page as HTML \\n\\niv. \\n\\nOn the product page, we need to find which HTML section contains the \\ncustomer comments. Let\\xe2\\x80\\x99s do inspect element(ctrl+shift+i) on the page first \\nto open the element-wise view of the HTML page. There we find the tag \\nwhich corresponds to the customer comments as shown below: \\n\\n16 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nPython code for implementing the same is: \\n\\n \\n\\ncommentboxes = prod_html.find_all(\\'div\\', {\\'class\\': \\n\"_3nrCtb\"}) # finding the HTML section containing the \\ncustomer comments \\n \\nOnce we have the list of all the comments, we now shall extract the \\ncustomer name(in grey), the rating(in green), comment heading(marked in \\nred), and the customer comment( highlighted in yellow) from the tag. \\n\\nv. \\n\\n \\n \\nThe Python code for the same is: \\n\\n \\n\\nreviews = [] # initializing an empty list for reviews \\n#  iterating over the comment section to get the details of \\nthe customer and their comments \\nfor commentbox in commentboxes: \\n\\n17 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nvi. \\n\\n    try: \\n        name = commentbox.div.div.find_all(\\'p\\', {\\'class\\': \\n\\'_3LYOAd _3sxSiS\\'})[0].text \\n \\n    except: \\n        name = \\'No Name\\' \\n \\n    try: \\n        rating = commentbox.div.div.div.div.text \\n \\n    except: \\n        rating = \\'No Rating\\' \\n \\n    try: \\n        commentHead = commentbox.div.div.div.p.text \\n    except: \\n        commentHead = \\'No Comment Heading\\' \\n    try: \\n        comtag = commentbox.div.div.find_all(\\'div\\', \\n{\\'class\\': \\'\\'}) \\n        custComment = comtag[0].div.text \\n    except: \\n        custComment = \\'No Customer Comment\\' \\nIf you notice, the parsing is done using the try-except blocks. It is done to \\nhandle the exception cases. If there is an exception in parsing the tag, we\\xe2\\x80\\x99ll \\ninsert a default string in that place. \\nOnce we have the details, we\\xe2\\x80\\x99ll insert them into MongoDB. After that, \\nwe\\xe2\\x80\\x99ll return the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 page as the response to the user containing all \\nthe reviews.  The python code for that is: \\n\\nmydict = {\"Product\": searchString, \"Name\": name, \"Rating\": \\nrating, \"CommentHead\": commentHead, \\n              \"Comment\": custComment} # saving that detail \\nto a dictionary \\n    x = table.insert_one(mydict) #insertig the dictionary \\ncontaining the rview comments to the collection \\n    reviews.append(mydict) #  appending the comments to the \\nreview list \\nreturn render_template(\\'results.html\\', reviews=reviews) # \\nshowing the review to the user \\n\\ne)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor reviews as shown below: \\n\\n \\n\\nHome Page: \\n\\n18 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nSearch Results: \\n\\n \\n\\n \\n\\n \\n\\n5. Heroku: \\n\\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0cWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n\\n6. Heroku Basics: \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n \\n\\nhave one. \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xe2\\x80\\xa2  Double-click the installation file and the following window shall appear: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n20 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on install to complete the installation. \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\nd)  We have created a new file \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the review scrapper folder: \\n\\ni.  Remove the first_flask.app file from the directory. Resulting folder \\n\\n \\n\\nstructure: \\n\\nii.  A default route has been added to the app.py file to direct  to the home page \\n\\nwhen the application is initially invoked as shown below: \\n\\n \\n\\n@app.route(\\'/\\',methods=[\\'GET\\'])  # route to display the home \\npage \\n@cross_origin() \\ndef homePage(): \\n    return render_template(\"index.html\") \\n\\niii.  We have removed the part where we were writing to MongoDB. \\n\\nConsuming MongoDB might incur charges. So, we have removed that part.  \\n\\n \\n\\niv.   \\n\\n \\n\\n22 | P a g e  \\n\\n\\x0c8. Heroku app creation and deployment \\n\\nb.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nc.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\n \\n\\nbelow: \\n\\nd.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\ne.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\nf.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\ng.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\nh.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\ni.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\nj.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nk.  After deployment, heroku gives you the URL to hit the web API. \\nl.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n23 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nFinal Result:  \\n\\n \\n \\n\\n \\n\\n \\n \\nThank You! \\n\\n \\n\\n24 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nImage Scrapper from \\nscratch to proudction \\n\\n      \\n\\n      \\n\\n      \\n\\n\\x0c \\n\\nTable of Contents \\nPreface .................................................................................................................................................... 2 \\n\\nIntroduction ............................................................................................................................................ 3 \\n\\nPrerequesites .......................................................................................................................................... 4 \\n\\nPyCharm Installation ................................................................................................................. 4 \\n\\nMongoDB Installation ............................................................................................................... 7 \\n\\nMongoDB Installation ............................................................................................................... 7 \\n\\nStarting MongoDB  .................................................................................................................. 10 \\n\\nApplication Architecture ...................................................................................................................... 12 \\n\\nImplementation in Python ................................................................................................................... 12 \\n\\nHeroku .................................................................................................................................................. 17 \\n\\nHeroku Basics ....................................................................................................................................... 17 \\n\\nSteps before cloud deployment ........................................................................................................... 19 \\n\\nHeroku app creation and deploying the app to cloud ........................................................................ 20 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n\\x0c \\n\\nPreface \\n\\nThis book is intended to help all the data scientists out there. It is a step by step guide \\nfor creating a web scraper, in this case, an image scrapper right from scratch and then \\ndeploying it to the heroku cloud platform. Image scrappers are extensively used in the \\nindustry today for collecting a huge number of images that are used as inputs for \\ntraining the object detection/classification/identification models. This book takes a \\nsimple example of an online image search and tries to explain the concepts simply, \\nextensively, and thoroughly to create a review scrapper right from scratch and then its \\ndeployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\nthem. \\n\\nWeb Scraping(Images) \\n\\n1. Introduction: \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xef\\x82\\xb7  Web Crawling\\xef\\x83\\xa0 Accessing the webpages over the internet and pulling data from \\n\\n\\xef\\x82\\xb7  HTML Parsing\\xef\\x83\\xa0 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\n\\xef\\x82\\xb7  We go to a website that shows images, and then we enter the keyword to search for the \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we search for images online? \\n\\nphotos. \\n\\n\\xef\\x82\\xb7  The website shows us the images. \\n\\xef\\x82\\xb7  We decide to download the image(s). \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nLet\\xe2\\x80\\x99s take an example where we need thousands of images to train our object \\ndetection/classification model. It is an excellent use case for implementing an image scraper. \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of searching for images and downloading them and \\ntake it further. \\n\\nFor example, if we open google.com and search for \\xe2\\x80\\x98cars, the search result will be as follows: \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n \\n\\nOur end goal is to build a web scraper that collects the images for a keyword from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xef\\x82\\xb7  Python installed. \\n\\xef\\x82\\xb7  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xef\\x82\\xb7  Flask Installed. (A simple command: pip install flask) \\n\\xef\\x82\\xb7  MongoDB installed (Explained Later). \\n\\xef\\x82\\xb7  Basic understanding of Python and HTML. \\n\\xef\\x82\\xb7  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n \\n \\n\\n \\n\\n \\n\\n2.1  PyCharm Installation: \\n\\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\n4 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\n \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n4. Implementation in Python: \\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98ImageScrapper\\xe2\\x80\\x99 on our local machines. \\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\n\\n\\xe2\\x80\\x98imagescrapper\\xe2\\x80\\x99,\\xe2\\x80\\x99imagescrapperservice\\xe2\\x80\\x99,\\xe2\\x80\\x99imagescrapperutils\\xe2\\x80\\x99, \\xe2\\x80\\x99static\\xe2\\x80\\x99, and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 \\n\\n \\n\\n12 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\nto hold the code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\n\\xe2\\x80\\x98css\\xe2\\x80\\x99 for keeping the stylesheets for our UI. \\n\\n3.  Inside \\xe2\\x80\\x98imagescrapper\\xe2\\x80\\x99,\\xe2\\x80\\x99imagescrapperservice\\xe2\\x80\\x99, and \\xe2\\x80\\x99imagescrapperutils\\xe2\\x80\\x99 create the \\nfiles \\xe2\\x80\\x98ImageScrapper.py\\xe2\\x80\\x99, \\xe2\\x80\\x98ImageScrapperService.py\\xe2\\x80\\x99 and \\xe2\\x80\\x98ImageScrapperUtils.py\\xe2\\x80\\x99 \\nrespectively. The files are attached here for reference. \\n\\n4.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98ImageScrapper\\xe2\\x80\\x99 folder. The file is attached \\n\\n \\n\\nhere for reference. \\n\\n5.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the file: \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The file is attached here for reference.  \\n\\n \\n\\n \\n\\n6.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x99index.html\\xe2\\x80\\x99, and \\n\\n\\xe2\\x80\\x98showImage.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xef\\x82\\xb7  index.html\\xef\\x83\\xa0  Home page of our application. \\n\\xef\\x82\\xb7  showImage.html\\xef\\x83\\xa0 Page to show the images for the searched keyword. \\n\\n7.   Now, the folder structure should look like: \\n\\n \\n\\n8.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\n \\n\\n \\n\\n13 | P a g e  \\n\\napp.pyindex.htmlshowImage.html\\x0c \\n\\nbutton. \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\nc)  The application now searches for images and shows the result on the \\n\\n\\xe2\\x80\\x98showImage.html\\xe2\\x80\\x99 page. \\n\\n9.  Understanding \\xe2\\x80\\x98app.py\\xe2\\x80\\x99. \\n\\na)  Import the necessary libraries: \\n\\nfrom flask_cors import CORS,cross_origin \\nfrom imagescrapperservice.ImageScrapperService import \\nImageScrapperService \\nfrom imagescrapper.ImageScrapper import ImageScrapper \\nfrom flask import Flask, render_template, request,jsonify \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\nroute path, the control gets transferred inside the application. Let\\xe2\\x80\\x99s  understand the \\nvarious routes: \\ni. \\n\\nThe route for redirecting to home page \\n\\n@app.route(\\'/\\')  # route for redirecting to the home \\npage \\n@cross_origin() \\ndef home(): \\n    return render_template(\\'index.html\\') \\nii. \\n\\nShowing the images on the screen once our parser successfully \\ngives the list of images. \\n\\n@app.route(\\'/showImages\\') # route to show the images on \\na webpage \\n@cross_origin() \\ndef show_images(): \\n    scraper_object=ImageScrapper() #Instantiating the \\nobject of class ImageScrapper \\n    \\nlist_of_jpg_files=scraper_object.list_only_jpg_files(\\'st\\natic\\') # obtaining the list of image files from the \\nstatic folder \\n    print(list_of_jpg_files) \\n    try: \\n        if(len(list_of_jpg_files)>0): # if there are \\nimages present, show them on a wen UI \\n            return \\nrender_template(\\'showImage.html\\',user_images = \\nlist_of_jpg_files) \\n        else: \\n            return \"Please try with a different string\" \\n# show this error message if no images are present in \\n\\n14 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nthe static folder \\n    except Exception as e: \\n        print(\\'no Images found \\', e) \\n        return \"Please try with a different string\" \\niii. \\nThe route to actually scrape the web for images and then \\npreparing the list of images and then calling the method in step \\n(ii) to show the images to the user. \\n\\n@app.route(\\'/searchImages\\', methods=[\\'GET\\',\\'POST\\']) \\ndef searchImages(): \\n    if request.method == \\'POST\\': \\n        print(\"entered post\") \\n        keyWord = request.form[\\'keyword\\'] # assigning \\nthe value of the input keyword to the variable keyword \\n \\n    else: \\n        print(\"did not enter post\") \\n    print(\\'printing = \\' + keyWord) \\n \\n    scraper_object = ImageScrapper() # instantiating the \\nclass \\n    list_of_jpg_files = \\nscraper_object.list_only_jpg_files(\\'static\\') # obtaining \\nthe list of image files from the static folder \\n    \\nscraper_object.delete_existing_image(list_of_jpg_files) \\n# deleting the old image files stored from the previous \\nsearch \\n    # splitting and combining the keyword for a string \\ncontaining multiple words \\n    image_name = keyWord.split() \\n    image_name = \\'+\\'.join(image_name) \\n    # adding the header metadata \\n    header = { \\n        \\'User-Agent\\': \"Mozilla/5.0 (Windows NT 6.1; \\nWOW64) AppleWebKit/537.36 (KHTML, like Gecko) \\nChrome/43.0.2357.134 Safari/537.36\"} \\n \\n    service = ImageScrapperService  # instantiating the \\nobject of class ImageScrapperService \\n    # (imageURLList, header, keyWord, fileLoc) \\n    masterListOfImages = service.downloadImages(keyWord, \\nheader) # getting the master list from keyword \\n    imageList = masterListOfImages[0] # extracting the \\nlist of images from the master list \\n    imageTypeList = masterListOfImages[1] # extracting \\nthe list of type of images from the masterlist \\n \\n    response = \"We have downloaded \", len(imageList), \\n\"images of \" + image_name + \" for you\" \\n \\n    return show_images() # redirect the control to the \\nshow images method \\niv. \\n\\nThe route to send the list of image URLs when the API is not \\ncalled from a web browser. \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c@app.route(\\'/api/showImages\\', methods=[\\'GET\\',\\'POST\\']) \\n# route to return the list of file locations for API \\ncalls \\n@cross_origin() \\ndef get_image_url(): \\n    if request.method == \\'POST\\': \\n        print(\"entered post\") \\n        keyWord =  request.json[\\'keyword\\'] # assigning \\nthe value of the input keyword to the variable keyword \\n \\n    else: \\n        print(\"Did not enter  post\") \\n    print(\\'printing = \\' + keyWord) \\n    # splitting and combining the keyword for a string \\ncontaining multiple words \\n    image_name = keyWord.split() \\n    image_name = \\'+\\'.join(image_name) \\n    # adding the header metadata \\n    header = { \\n        \\'User-Agent\\': \"Mozilla/5.0 (Windows NT 6.1; \\nWOW64) AppleWebKit/537.36 (KHTML, like Gecko) \\nChrome/43.0.2357.134 Safari/537.36\"} \\n \\n    service = ImageScrapperService # instantiating the \\nobject of class ImageScrapperService \\n    url_enum = service.get_image_urls(keyWord, header) \\n# getting the URL enumeration \\n    url_list=[] # initializing and empty url list \\n    for i, (img, Type) in enumerate(url_enum[0:5]): \\n        # creating key value pairs of image URLs to be \\nsent as json \\n        dict={\\'image_url\\':img} \\n        url_list.append(dict) \\n    return jsonify(url_list) # send the url list in \\nJSON format \\n\\n  \\n\\nd)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor images as shown below: \\nHome Page: \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n16 | P a g e  \\n\\n\\x0c \\n\\nSearch Results: \\n\\n \\n\\n \\n5. Heroku: \\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\nWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n6. Heroku Basics: \\n\\n\\xef\\x82\\xb7  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n\\xef\\x82\\xb7  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xef\\x82\\xb7  Double-click the installation file and the following window shall appear: \\n\\nhave one. \\n\\n \\n\\n \\n\\n17 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n\\xef\\x82\\xb7  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n\\xef\\x82\\xb7  Click on install to complete the installation. \\n\\n \\n\\n \\n\\n18 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0c \\n\\nd)  A change has been made to app.py. The part where the port  number for local machine was \\nprovided has been commented and the part without port number has been uncommented to \\nrun on the cloud. \\nif __name__ == \"__main__\": \\n    #app.run(host=\\'127.0.0.1\\', port=8000) # port to run on local \\nmachine \\n    app.run(debug=True) # to run on cloud \\n  \\n8. Heroku app creation and deploying the app to \\n\\ncloud: \\na.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nb.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nbelow: \\n\\nc.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\nd.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\ne.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\nf.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\ng.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\nh.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\ni.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nj.  After deployment, heroku gives you the URL to hit the web API. \\nk.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n20 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0cFinal Result:  \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nThank You! \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n                                                 \\n\\n                                                                   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nReview scraper from \\nscratch till deployment \\n\\nvirat Sagar \\n\\n \\n\\n[Date] \\n\\n[Course title] \\n\\n\\x0c \\n\\nTable of Contents \\nPreface .................................................................................................................................................... 2 \\n\\nIntroduction ............................................................................................................................................ 3 \\n\\nPrerequisites ........................................................................................................................................... 5 \\n\\nPyCharm Installation ................................................................................................................. 6 \\n\\nMongoDB Installation ............................................................................................................... 8 \\n\\nStarting MongoDB  .................................................................................................................. 11 \\n\\nApplication Architecture ...................................................................................................................... 13 \\n\\nPython Implementation ....................................................................................................................... 13 \\n\\nHeroku .................................................................................................................................................. 19 \\n\\nHeroku Basics ....................................................................................................................................... 20 \\n\\nSteps before cloud deployment ........................................................................................................... 21 \\n\\nHeroku app creation and deployment ................................................................................................ 23 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nPreface \\n\\nThis book is intended towards helping all the data scientists out there. It is a step by \\nstep guide for creating a web scraper, in this case, a review scrapper right from scratch \\nand then deploying it to the heroku cloud platform. Text scrappers are extensively used \\nin the industry today for competitive pricing, market studies, customer sentiment \\nanalysis, etc. This book takes a simple example of an online cell phone purchase and \\ntries to explain the concepts simply, extensively, and thoroughly to create a review \\nscrapper right from scratch and then its deployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n1. Introduction: \\n\\nWeb Scraping(Text) \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xe2\\x80\\xa2  Web Crawling\\xe2\\x86\\x92 Accessing the webpages over the internet and pulling data from \\n\\nthem. \\n\\n\\xe2\\x80\\xa2  HTML Parsing\\xe2\\x86\\x92 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we buy a phone online? \\n\\n1.  We first look for a phone with good reviews \\n2.  We see on which website it\\xe2\\x80\\x99s available at the lowest price \\n3.  We check whether it\\xe2\\x80\\x99s  delivered in our area or not \\n4.  If everything looks good, then we buy the phone. \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nOther examples of the applications of web scrapping are: \\n\\n\\xe2\\x80\\xa2  Competitive pricing. \\n\\xe2\\x80\\xa2  Manufacturers monitor the market, whether the retailer is maintaining a minimum price \\n\\n\\xe2\\x80\\xa2  Sentiment analysis of the consumers, whether they are happy with the services and \\n\\nor not. \\n\\nproducts or not. \\n\\n\\xe2\\x80\\xa2  To aggregate news articles. \\n\\xe2\\x80\\xa2  To aggregate Marketing data. \\n\\xe2\\x80\\xa2  To gain financial insights from the market. \\n\\xe2\\x80\\xa2  To gather data for research. \\n\\xe2\\x80\\xa2  To generate marketing leads. \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  To collect trending topics by media houses. \\n\\nAnd, the list goes on.  \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of buying a phone online further and try to scrap the \\nreviews from the website about the phone that we are planning to buy. \\n\\nFor example, if we open filpkart.com and search for \\xe2\\x80\\x98iPhone\\xe2\\x80\\x99, the search result will be as \\nfollows: \\n \\n\\nThen if we click on a product link, it will take us to to the following page: \\n\\n \\n\\n \\n\\n4 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\nIf we scroll down on this page, we\\xe2\\x80\\x99ll get to see the comments posted by the customers: \\n\\n \\n\\nOur end goal is to build a web scraper that collects the reviews of a product from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xe2\\x80\\xa2  Python installed. \\n\\xe2\\x80\\xa2  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xe2\\x80\\xa2  Flask Installed. (A simple command: pip install flask) \\n\\xe2\\x80\\xa2  MongoDB installed (Explained Later). \\n\\xe2\\x80\\xa2  Basic understanding of Python and HTML. \\n\\xe2\\x80\\xa2  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n2.1  PyCharm Installation: \\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n12 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n              \\n \\n\\n \\n\\n \\n\\n4. Python Implementation: \\n\\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 on our local machines. \\n\\n \\n\\n13 | P a g e  \\n\\n\\x0c \\n\\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\xe2\\x80\\x98static\\xe2\\x80\\x99 and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 to hold \\nthe code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98css\\xe2\\x80\\x99 \\nfor keeping the stylesheets for our UI. \\n\\n3.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98flask_app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder.  \\n4.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the files: \\xe2\\x80\\x98main.css\\xe2\\x80\\x99 and \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The files are attached \\n\\nhere for reference.  \\n\\n5.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x98base.html\\xe2\\x80\\x99,\\xe2\\x80\\x99index.html\\xe2\\x80\\x99, \\n\\nand \\xe2\\x80\\x98results.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xe2\\x80\\xa2  base.html\\xe2\\x86\\x92 It acts as the common building block for the other two HTML \\n\\n \\n\\npages. \\n\\n\\xe2\\x80\\xa2  index.html\\xe2\\x86\\x92  Home page of our application. \\n\\xe2\\x80\\xa2  results.html\\xe2\\x86\\x92 Page to show the reviews for the searched keyword. \\n\\n6.   Now, the folder structure should look like: \\n\\n \\n\\n7.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\n \\n\\n14 | P a g e  \\n\\nbutton. \\n\\n \\n\\nbase.htmlindex.htmlresults.html\\x0cc)  The application now searches for reviews and shows the result on the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 \\n\\n \\n\\npage. \\n\\n8.  Understanding flask_app.py. \\n\\n \\na)  Import the necessary libraries: \\n\\nfrom flask import Flask, render_template, request,jsonify \\nfrom flask_cors import CORS,cross_origin \\nimport requests \\nfrom bs4 import BeautifulSoup as bs \\nfrom urllib.request import urlopen as uReq \\nimport pymongo \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\n\\nroute path, the control gets transferred inside the application. \\n\\n@app.route(\\'/\\',methods=[\\'POST\\',\\'GET\\']) # route with allowed \\nmethods as POST and GET \\n\\nd)  Now let\\xe2\\x80\\x99s understand the \\xe2\\x80\\x98index()\\xe2\\x80\\x99 function. \\n\\ni. \\n\\nIf the HTTP request method is POST(which is defined in index.html at \\nform submit action), then first check if the records for the searched \\nkeyword is already present in the database or not. If present, show that to \\nthe user. \\n\\nii. \\n\\ndbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \\n# opening a connection to Mongo \\ndb = dbConn[\\'crawlerDB\\'] # connecting to the database \\ncalled crawlerDB \\nreviews = db[searchString].find({}) # searching the \\ncollection with the name same as the keyword \\nif reviews.count() > 0: # if there is a collection with \\nsearched keyword and it has records in it \\n    return render_template(\\'results.html\\',reviews=reviews) \\n# show the results to user \\nIf the searched keyword doesn\\xe2\\x80\\x99t have a database entry, then the application \\ntries to fetch the details from the internet, as shown below: \\n\\nflipkart_url = \"https://www.flipkart.com/search?q=\" + \\nsearchString # preparing the URL to search the product on \\nFlipkart \\nuClient = uReq(flipkart_url) # requesting the webpage from \\nthe internet \\nflipkartPage = uClient.read() # reading the webpage \\nuClient.close() # closing the connection to the web server \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c \\n\\niii. \\n\\nflipkart_html = bs(flipkartPage, \"html.parser\") # parsing \\nthe webpage as HTML \\nOnce we have the entire HTML page, we try to get the product URL and \\nthen jump to the product page. It is similar to redirecting to the following \\npage: \\n\\nThe equivalent Python code is: \\n\\n \\n\\nproductLink = \"https://www.flipkart.com\" + \\nbox.div.div.div.a[\\'href\\'] # extracting the actual product \\nlink \\nprodRes = requests.get(productLink) # getting the product \\npage from server \\nprod_html = bs(prodRes.text, \"html.parser\") # parsing the \\nproduct page as HTML \\n\\niv. \\n\\nOn the product page, we need to find which HTML section contains the \\ncustomer comments. Let\\xe2\\x80\\x99s do inspect element(ctrl+shift+i) on the page first \\nto open the element-wise view of the HTML page. There we find the tag \\nwhich corresponds to the customer comments as shown below: \\n\\n16 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nPython code for implementing the same is: \\n\\n \\n\\ncommentboxes = prod_html.find_all(\\'div\\', {\\'class\\': \\n\"_3nrCtb\"}) # finding the HTML section containing the \\ncustomer comments \\n \\nOnce we have the list of all the comments, we now shall extract the \\ncustomer name(in grey), the rating(in green), comment heading(marked in \\nred), and the customer comment( highlighted in yellow) from the tag. \\n\\nv. \\n\\n \\n \\nThe Python code for the same is: \\n\\n \\n\\nreviews = [] # initializing an empty list for reviews \\n#  iterating over the comment section to get the details of \\nthe customer and their comments \\nfor commentbox in commentboxes: \\n\\n17 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nvi. \\n\\n    try: \\n        name = commentbox.div.div.find_all(\\'p\\', {\\'class\\': \\n\\'_3LYOAd _3sxSiS\\'})[0].text \\n \\n    except: \\n        name = \\'No Name\\' \\n \\n    try: \\n        rating = commentbox.div.div.div.div.text \\n \\n    except: \\n        rating = \\'No Rating\\' \\n \\n    try: \\n        commentHead = commentbox.div.div.div.p.text \\n    except: \\n        commentHead = \\'No Comment Heading\\' \\n    try: \\n        comtag = commentbox.div.div.find_all(\\'div\\', \\n{\\'class\\': \\'\\'}) \\n        custComment = comtag[0].div.text \\n    except: \\n        custComment = \\'No Customer Comment\\' \\nIf you notice, the parsing is done using the try-except blocks. It is done to \\nhandle the exception cases. If there is an exception in parsing the tag, we\\xe2\\x80\\x99ll \\ninsert a default string in that place. \\nOnce we have the details, we\\xe2\\x80\\x99ll insert them into MongoDB. After that, \\nwe\\xe2\\x80\\x99ll return the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 page as the response to the user containing all \\nthe reviews.  The python code for that is: \\n\\nmydict = {\"Product\": searchString, \"Name\": name, \"Rating\": \\nrating, \"CommentHead\": commentHead, \\n              \"Comment\": custComment} # saving that detail \\nto a dictionary \\n    x = table.insert_one(mydict) #insertig the dictionary \\ncontaining the rview comments to the collection \\n    reviews.append(mydict) #  appending the comments to the \\nreview list \\nreturn render_template(\\'results.html\\', reviews=reviews) # \\nshowing the review to the user \\n\\ne)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor reviews as shown below: \\n\\n \\n\\nHome Page: \\n\\n18 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nSearch Results: \\n\\n \\n\\n \\n\\n \\n\\n5. Heroku: \\n\\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0cWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n\\n6. Heroku Basics: \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n \\n\\nhave one. \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xe2\\x80\\xa2  Double-click the installation file and the following window shall appear: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n20 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on install to complete the installation. \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\nd)  We have created a new file \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the review scrapper folder: \\n\\ni.  Remove the first_flask.app file from the directory. Resulting folder \\n\\n \\n\\nstructure: \\n\\nii.  A default route has been added to the app.py file to direct  to the home page \\n\\nwhen the application is initially invoked as shown below: \\n\\n \\n\\n@app.route(\\'/\\',methods=[\\'GET\\'])  # route to display the home \\npage \\n@cross_origin() \\ndef homePage(): \\n    return render_template(\"index.html\") \\n\\niii.  We have removed the part where we were writing to MongoDB. \\n\\nConsuming MongoDB might incur charges. So, we have removed that part.  \\n\\n \\n\\niv.   \\n\\n \\n\\n22 | P a g e  \\n\\n\\x0c8. Heroku app creation and deployment \\n\\nb.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nc.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\n \\n\\nbelow: \\n\\nd.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\ne.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\nf.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\ng.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\nh.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\ni.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\nj.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nk.  After deployment, heroku gives you the URL to hit the web API. \\nl.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n23 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nFinal Result:  \\n\\n \\n \\n\\n \\n\\n \\n \\nThank You! \\n\\n \\n\\n24 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n                                                 \\n\\n                                                                   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nReview scraper from \\nscratch till deployment \\n\\nvirat Sagar \\n\\n \\n\\n[Date] \\n\\n[Course title] \\n\\n\\x0c \\n\\nTable of Contents \\nPreface .................................................................................................................................................... 2 \\n\\nIntroduction ............................................................................................................................................ 3 \\n\\nPrerequisites ........................................................................................................................................... 5 \\n\\nPyCharm Installation ................................................................................................................. 6 \\n\\nMongoDB Installation ............................................................................................................... 8 \\n\\nStarting MongoDB  .................................................................................................................. 11 \\n\\nApplication Architecture ...................................................................................................................... 13 \\n\\nPython Implementation ....................................................................................................................... 13 \\n\\nHeroku .................................................................................................................................................. 19 \\n\\nHeroku Basics ....................................................................................................................................... 20 \\n\\nSteps before cloud deployment ........................................................................................................... 21 \\n\\nHeroku app creation and deployment ................................................................................................ 23 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nPreface \\n\\nThis book is intended towards helping all the data scientists out there. It is a step by \\nstep guide for creating a web scraper, in this case, a review scrapper right from scratch \\nand then deploying it to the heroku cloud platform. Text scrappers are extensively used \\nin the industry today for competitive pricing, market studies, customer sentiment \\nanalysis, etc. This book takes a simple example of an online cell phone purchase and \\ntries to explain the concepts simply, extensively, and thoroughly to create a review \\nscrapper right from scratch and then its deployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n1. Introduction: \\n\\nWeb Scraping(Text) \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xe2\\x80\\xa2  Web Crawling\\xe2\\x86\\x92 Accessing the webpages over the internet and pulling data from \\n\\nthem. \\n\\n\\xe2\\x80\\xa2  HTML Parsing\\xe2\\x86\\x92 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we buy a phone online? \\n\\n1.  We first look for a phone with good reviews \\n2.  We see on which website it\\xe2\\x80\\x99s available at the lowest price \\n3.  We check whether it\\xe2\\x80\\x99s  delivered in our area or not \\n4.  If everything looks good, then we buy the phone. \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nOther examples of the applications of web scrapping are: \\n\\n\\xe2\\x80\\xa2  Competitive pricing. \\n\\xe2\\x80\\xa2  Manufacturers monitor the market, whether the retailer is maintaining a minimum price \\n\\n\\xe2\\x80\\xa2  Sentiment analysis of the consumers, whether they are happy with the services and \\n\\nor not. \\n\\nproducts or not. \\n\\n\\xe2\\x80\\xa2  To aggregate news articles. \\n\\xe2\\x80\\xa2  To aggregate Marketing data. \\n\\xe2\\x80\\xa2  To gain financial insights from the market. \\n\\xe2\\x80\\xa2  To gather data for research. \\n\\xe2\\x80\\xa2  To generate marketing leads. \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  To collect trending topics by media houses. \\n\\nAnd, the list goes on.  \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of buying a phone online further and try to scrap the \\nreviews from the website about the phone that we are planning to buy. \\n\\nFor example, if we open filpkart.com and search for \\xe2\\x80\\x98iPhone\\xe2\\x80\\x99, the search result will be as \\nfollows: \\n \\n\\nThen if we click on a product link, it will take us to to the following page: \\n\\n \\n\\n \\n\\n4 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\nIf we scroll down on this page, we\\xe2\\x80\\x99ll get to see the comments posted by the customers: \\n\\n \\n\\nOur end goal is to build a web scraper that collects the reviews of a product from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xe2\\x80\\xa2  Python installed. \\n\\xe2\\x80\\xa2  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xe2\\x80\\xa2  Flask Installed. (A simple command: pip install flask) \\n\\xe2\\x80\\xa2  MongoDB installed (Explained Later). \\n\\xe2\\x80\\xa2  Basic understanding of Python and HTML. \\n\\xe2\\x80\\xa2  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n2.1  PyCharm Installation: \\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n12 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n              \\n \\n\\n \\n\\n \\n\\n4. Python Implementation: \\n\\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 on our local machines. \\n\\n \\n\\n13 | P a g e  \\n\\n\\x0c \\n\\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\xe2\\x80\\x98static\\xe2\\x80\\x99 and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 to hold \\nthe code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98css\\xe2\\x80\\x99 \\nfor keeping the stylesheets for our UI. \\n\\n3.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98flask_app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder.  \\n4.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the files: \\xe2\\x80\\x98main.css\\xe2\\x80\\x99 and \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The files are attached \\n\\nhere for reference.  \\n\\n5.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x98base.html\\xe2\\x80\\x99,\\xe2\\x80\\x99index.html\\xe2\\x80\\x99, \\n\\nand \\xe2\\x80\\x98results.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xe2\\x80\\xa2  base.html\\xe2\\x86\\x92 It acts as the common building block for the other two HTML \\n\\n \\n\\npages. \\n\\n\\xe2\\x80\\xa2  index.html\\xe2\\x86\\x92  Home page of our application. \\n\\xe2\\x80\\xa2  results.html\\xe2\\x86\\x92 Page to show the reviews for the searched keyword. \\n\\n6.   Now, the folder structure should look like: \\n\\n \\n\\n7.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\n \\n\\n14 | P a g e  \\n\\nbutton. \\n\\n \\n\\nbase.htmlindex.htmlresults.html\\x0cc)  The application now searches for reviews and shows the result on the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 \\n\\n \\n\\npage. \\n\\n8.  Understanding flask_app.py. \\n\\n \\na)  Import the necessary libraries: \\n\\nfrom flask import Flask, render_template, request,jsonify \\nfrom flask_cors import CORS,cross_origin \\nimport requests \\nfrom bs4 import BeautifulSoup as bs \\nfrom urllib.request import urlopen as uReq \\nimport pymongo \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\n\\nroute path, the control gets transferred inside the application. \\n\\n@app.route(\\'/\\',methods=[\\'POST\\',\\'GET\\']) # route with allowed \\nmethods as POST and GET \\n\\nd)  Now let\\xe2\\x80\\x99s understand the \\xe2\\x80\\x98index()\\xe2\\x80\\x99 function. \\n\\ni. \\n\\nIf the HTTP request method is POST(which is defined in index.html at \\nform submit action), then first check if the records for the searched \\nkeyword is already present in the database or not. If present, show that to \\nthe user. \\n\\nii. \\n\\ndbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \\n# opening a connection to Mongo \\ndb = dbConn[\\'crawlerDB\\'] # connecting to the database \\ncalled crawlerDB \\nreviews = db[searchString].find({}) # searching the \\ncollection with the name same as the keyword \\nif reviews.count() > 0: # if there is a collection with \\nsearched keyword and it has records in it \\n    return render_template(\\'results.html\\',reviews=reviews) \\n# show the results to user \\nIf the searched keyword doesn\\xe2\\x80\\x99t have a database entry, then the application \\ntries to fetch the details from the internet, as shown below: \\n\\nflipkart_url = \"https://www.flipkart.com/search?q=\" + \\nsearchString # preparing the URL to search the product on \\nFlipkart \\nuClient = uReq(flipkart_url) # requesting the webpage from \\nthe internet \\nflipkartPage = uClient.read() # reading the webpage \\nuClient.close() # closing the connection to the web server \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c \\n\\niii. \\n\\nflipkart_html = bs(flipkartPage, \"html.parser\") # parsing \\nthe webpage as HTML \\nOnce we have the entire HTML page, we try to get the product URL and \\nthen jump to the product page. It is similar to redirecting to the following \\npage: \\n\\nThe equivalent Python code is: \\n\\n \\n\\nproductLink = \"https://www.flipkart.com\" + \\nbox.div.div.div.a[\\'href\\'] # extracting the actual product \\nlink \\nprodRes = requests.get(productLink) # getting the product \\npage from server \\nprod_html = bs(prodRes.text, \"html.parser\") # parsing the \\nproduct page as HTML \\n\\niv. \\n\\nOn the product page, we need to find which HTML section contains the \\ncustomer comments. Let\\xe2\\x80\\x99s do inspect element(ctrl+shift+i) on the page first \\nto open the element-wise view of the HTML page. There we find the tag \\nwhich corresponds to the customer comments as shown below: \\n\\n16 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nPython code for implementing the same is: \\n\\n \\n\\ncommentboxes = prod_html.find_all(\\'div\\', {\\'class\\': \\n\"_3nrCtb\"}) # finding the HTML section containing the \\ncustomer comments \\n \\nOnce we have the list of all the comments, we now shall extract the \\ncustomer name(in grey), the rating(in green), comment heading(marked in \\nred), and the customer comment( highlighted in yellow) from the tag. \\n\\nv. \\n\\n \\n \\nThe Python code for the same is: \\n\\n \\n\\nreviews = [] # initializing an empty list for reviews \\n#  iterating over the comment section to get the details of \\nthe customer and their comments \\nfor commentbox in commentboxes: \\n\\n17 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nvi. \\n\\n    try: \\n        name = commentbox.div.div.find_all(\\'p\\', {\\'class\\': \\n\\'_3LYOAd _3sxSiS\\'})[0].text \\n \\n    except: \\n        name = \\'No Name\\' \\n \\n    try: \\n        rating = commentbox.div.div.div.div.text \\n \\n    except: \\n        rating = \\'No Rating\\' \\n \\n    try: \\n        commentHead = commentbox.div.div.div.p.text \\n    except: \\n        commentHead = \\'No Comment Heading\\' \\n    try: \\n        comtag = commentbox.div.div.find_all(\\'div\\', \\n{\\'class\\': \\'\\'}) \\n        custComment = comtag[0].div.text \\n    except: \\n        custComment = \\'No Customer Comment\\' \\nIf you notice, the parsing is done using the try-except blocks. It is done to \\nhandle the exception cases. If there is an exception in parsing the tag, we\\xe2\\x80\\x99ll \\ninsert a default string in that place. \\nOnce we have the details, we\\xe2\\x80\\x99ll insert them into MongoDB. After that, \\nwe\\xe2\\x80\\x99ll return the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 page as the response to the user containing all \\nthe reviews.  The python code for that is: \\n\\nmydict = {\"Product\": searchString, \"Name\": name, \"Rating\": \\nrating, \"CommentHead\": commentHead, \\n              \"Comment\": custComment} # saving that detail \\nto a dictionary \\n    x = table.insert_one(mydict) #insertig the dictionary \\ncontaining the rview comments to the collection \\n    reviews.append(mydict) #  appending the comments to the \\nreview list \\nreturn render_template(\\'results.html\\', reviews=reviews) # \\nshowing the review to the user \\n\\ne)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor reviews as shown below: \\n\\n \\n\\nHome Page: \\n\\n18 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nSearch Results: \\n\\n \\n\\n \\n\\n \\n\\n5. Heroku: \\n\\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0cWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n\\n6. Heroku Basics: \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n \\n\\nhave one. \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xe2\\x80\\xa2  Double-click the installation file and the following window shall appear: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n20 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on install to complete the installation. \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\nd)  We have created a new file \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the review scrapper folder: \\n\\ni.  Remove the first_flask.app file from the directory. Resulting folder \\n\\n \\n\\nstructure: \\n\\nii.  A default route has been added to the app.py file to direct  to the home page \\n\\nwhen the application is initially invoked as shown below: \\n\\n \\n\\n@app.route(\\'/\\',methods=[\\'GET\\'])  # route to display the home \\npage \\n@cross_origin() \\ndef homePage(): \\n    return render_template(\"index.html\") \\n\\niii.  We have removed the part where we were writing to MongoDB. \\n\\nConsuming MongoDB might incur charges. So, we have removed that part.  \\n\\n \\n\\niv.   \\n\\n \\n\\n22 | P a g e  \\n\\n\\x0c8. Heroku app creation and deployment \\n\\nb.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nc.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\n \\n\\nbelow: \\n\\nd.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\ne.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\nf.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\ng.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\nh.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\ni.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\nj.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nk.  After deployment, heroku gives you the URL to hit the web API. \\nl.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n23 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nFinal Result:  \\n\\n \\n \\n\\n \\n\\n \\n \\nThank You! \\n\\n \\n\\n24 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n                                                 \\n\\n                                                                   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nReview scraper from \\nscratch till deployment \\n\\nvirat Sagar \\n\\n \\n\\n[Date] \\n\\n[Course title] \\n\\n\\x0c \\n\\nTable of Contents \\nPreface .................................................................................................................................................... 2 \\n\\nIntroduction ............................................................................................................................................ 3 \\n\\nPrerequisites ........................................................................................................................................... 5 \\n\\nPyCharm Installation ................................................................................................................. 6 \\n\\nMongoDB Installation ............................................................................................................... 8 \\n\\nStarting MongoDB  .................................................................................................................. 11 \\n\\nApplication Architecture ...................................................................................................................... 13 \\n\\nPython Implementation ....................................................................................................................... 13 \\n\\nHeroku .................................................................................................................................................. 19 \\n\\nHeroku Basics ....................................................................................................................................... 20 \\n\\nSteps before cloud deployment ........................................................................................................... 21 \\n\\nHeroku app creation and deployment ................................................................................................ 23 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nPreface \\n\\nThis book is intended towards helping all the data scientists out there. It is a step by \\nstep guide for creating a web scraper, in this case, a review scrapper right from scratch \\nand then deploying it to the heroku cloud platform. Text scrappers are extensively used \\nin the industry today for competitive pricing, market studies, customer sentiment \\nanalysis, etc. This book takes a simple example of an online cell phone purchase and \\ntries to explain the concepts simply, extensively, and thoroughly to create a review \\nscrapper right from scratch and then its deployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n1. Introduction: \\n\\nWeb Scraping(Text) \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xe2\\x80\\xa2  Web Crawling\\xe2\\x86\\x92 Accessing the webpages over the internet and pulling data from \\n\\nthem. \\n\\n\\xe2\\x80\\xa2  HTML Parsing\\xe2\\x86\\x92 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we buy a phone online? \\n\\n1.  We first look for a phone with good reviews \\n2.  We see on which website it\\xe2\\x80\\x99s available at the lowest price \\n3.  We check whether it\\xe2\\x80\\x99s  delivered in our area or not \\n4.  If everything looks good, then we buy the phone. \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nOther examples of the applications of web scrapping are: \\n\\n\\xe2\\x80\\xa2  Competitive pricing. \\n\\xe2\\x80\\xa2  Manufacturers monitor the market, whether the retailer is maintaining a minimum price \\n\\n\\xe2\\x80\\xa2  Sentiment analysis of the consumers, whether they are happy with the services and \\n\\nor not. \\n\\nproducts or not. \\n\\n\\xe2\\x80\\xa2  To aggregate news articles. \\n\\xe2\\x80\\xa2  To aggregate Marketing data. \\n\\xe2\\x80\\xa2  To gain financial insights from the market. \\n\\xe2\\x80\\xa2  To gather data for research. \\n\\xe2\\x80\\xa2  To generate marketing leads. \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  To collect trending topics by media houses. \\n\\nAnd, the list goes on.  \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of buying a phone online further and try to scrap the \\nreviews from the website about the phone that we are planning to buy. \\n\\nFor example, if we open filpkart.com and search for \\xe2\\x80\\x98iPhone\\xe2\\x80\\x99, the search result will be as \\nfollows: \\n \\n\\nThen if we click on a product link, it will take us to to the following page: \\n\\n \\n\\n \\n\\n4 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\nIf we scroll down on this page, we\\xe2\\x80\\x99ll get to see the comments posted by the customers: \\n\\n \\n\\nOur end goal is to build a web scraper that collects the reviews of a product from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xe2\\x80\\xa2  Python installed. \\n\\xe2\\x80\\xa2  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xe2\\x80\\xa2  Flask Installed. (A simple command: pip install flask) \\n\\xe2\\x80\\xa2  MongoDB installed (Explained Later). \\n\\xe2\\x80\\xa2  Basic understanding of Python and HTML. \\n\\xe2\\x80\\xa2  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n2.1  PyCharm Installation: \\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n12 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n              \\n \\n\\n \\n\\n \\n\\n4. Python Implementation: \\n\\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 on our local machines. \\n\\n \\n\\n13 | P a g e  \\n\\n\\x0c \\n\\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\xe2\\x80\\x98static\\xe2\\x80\\x99 and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 to hold \\nthe code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98css\\xe2\\x80\\x99 \\nfor keeping the stylesheets for our UI. \\n\\n3.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98flask_app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder.  \\n4.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the files: \\xe2\\x80\\x98main.css\\xe2\\x80\\x99 and \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The files are attached \\n\\nhere for reference.  \\n\\n5.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x98base.html\\xe2\\x80\\x99,\\xe2\\x80\\x99index.html\\xe2\\x80\\x99, \\n\\nand \\xe2\\x80\\x98results.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xe2\\x80\\xa2  base.html\\xe2\\x86\\x92 It acts as the common building block for the other two HTML \\n\\n \\n\\npages. \\n\\n\\xe2\\x80\\xa2  index.html\\xe2\\x86\\x92  Home page of our application. \\n\\xe2\\x80\\xa2  results.html\\xe2\\x86\\x92 Page to show the reviews for the searched keyword. \\n\\n6.   Now, the folder structure should look like: \\n\\n \\n\\n7.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\n \\n\\n14 | P a g e  \\n\\nbutton. \\n\\n \\n\\nbase.htmlindex.htmlresults.html\\x0cc)  The application now searches for reviews and shows the result on the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 \\n\\n \\n\\npage. \\n\\n8.  Understanding flask_app.py. \\n\\n \\na)  Import the necessary libraries: \\n\\nfrom flask import Flask, render_template, request,jsonify \\nfrom flask_cors import CORS,cross_origin \\nimport requests \\nfrom bs4 import BeautifulSoup as bs \\nfrom urllib.request import urlopen as uReq \\nimport pymongo \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\n\\nroute path, the control gets transferred inside the application. \\n\\n@app.route(\\'/\\',methods=[\\'POST\\',\\'GET\\']) # route with allowed \\nmethods as POST and GET \\n\\nd)  Now let\\xe2\\x80\\x99s understand the \\xe2\\x80\\x98index()\\xe2\\x80\\x99 function. \\n\\ni. \\n\\nIf the HTTP request method is POST(which is defined in index.html at \\nform submit action), then first check if the records for the searched \\nkeyword is already present in the database or not. If present, show that to \\nthe user. \\n\\nii. \\n\\ndbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \\n# opening a connection to Mongo \\ndb = dbConn[\\'crawlerDB\\'] # connecting to the database \\ncalled crawlerDB \\nreviews = db[searchString].find({}) # searching the \\ncollection with the name same as the keyword \\nif reviews.count() > 0: # if there is a collection with \\nsearched keyword and it has records in it \\n    return render_template(\\'results.html\\',reviews=reviews) \\n# show the results to user \\nIf the searched keyword doesn\\xe2\\x80\\x99t have a database entry, then the application \\ntries to fetch the details from the internet, as shown below: \\n\\nflipkart_url = \"https://www.flipkart.com/search?q=\" + \\nsearchString # preparing the URL to search the product on \\nFlipkart \\nuClient = uReq(flipkart_url) # requesting the webpage from \\nthe internet \\nflipkartPage = uClient.read() # reading the webpage \\nuClient.close() # closing the connection to the web server \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c \\n\\niii. \\n\\nflipkart_html = bs(flipkartPage, \"html.parser\") # parsing \\nthe webpage as HTML \\nOnce we have the entire HTML page, we try to get the product URL and \\nthen jump to the product page. It is similar to redirecting to the following \\npage: \\n\\nThe equivalent Python code is: \\n\\n \\n\\nproductLink = \"https://www.flipkart.com\" + \\nbox.div.div.div.a[\\'href\\'] # extracting the actual product \\nlink \\nprodRes = requests.get(productLink) # getting the product \\npage from server \\nprod_html = bs(prodRes.text, \"html.parser\") # parsing the \\nproduct page as HTML \\n\\niv. \\n\\nOn the product page, we need to find which HTML section contains the \\ncustomer comments. Let\\xe2\\x80\\x99s do inspect element(ctrl+shift+i) on the page first \\nto open the element-wise view of the HTML page. There we find the tag \\nwhich corresponds to the customer comments as shown below: \\n\\n16 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nPython code for implementing the same is: \\n\\n \\n\\ncommentboxes = prod_html.find_all(\\'div\\', {\\'class\\': \\n\"_3nrCtb\"}) # finding the HTML section containing the \\ncustomer comments \\n \\nOnce we have the list of all the comments, we now shall extract the \\ncustomer name(in grey), the rating(in green), comment heading(marked in \\nred), and the customer comment( highlighted in yellow) from the tag. \\n\\nv. \\n\\n \\n \\nThe Python code for the same is: \\n\\n \\n\\nreviews = [] # initializing an empty list for reviews \\n#  iterating over the comment section to get the details of \\nthe customer and their comments \\nfor commentbox in commentboxes: \\n\\n17 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nvi. \\n\\n    try: \\n        name = commentbox.div.div.find_all(\\'p\\', {\\'class\\': \\n\\'_3LYOAd _3sxSiS\\'})[0].text \\n \\n    except: \\n        name = \\'No Name\\' \\n \\n    try: \\n        rating = commentbox.div.div.div.div.text \\n \\n    except: \\n        rating = \\'No Rating\\' \\n \\n    try: \\n        commentHead = commentbox.div.div.div.p.text \\n    except: \\n        commentHead = \\'No Comment Heading\\' \\n    try: \\n        comtag = commentbox.div.div.find_all(\\'div\\', \\n{\\'class\\': \\'\\'}) \\n        custComment = comtag[0].div.text \\n    except: \\n        custComment = \\'No Customer Comment\\' \\nIf you notice, the parsing is done using the try-except blocks. It is done to \\nhandle the exception cases. If there is an exception in parsing the tag, we\\xe2\\x80\\x99ll \\ninsert a default string in that place. \\nOnce we have the details, we\\xe2\\x80\\x99ll insert them into MongoDB. After that, \\nwe\\xe2\\x80\\x99ll return the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 page as the response to the user containing all \\nthe reviews.  The python code for that is: \\n\\nmydict = {\"Product\": searchString, \"Name\": name, \"Rating\": \\nrating, \"CommentHead\": commentHead, \\n              \"Comment\": custComment} # saving that detail \\nto a dictionary \\n    x = table.insert_one(mydict) #insertig the dictionary \\ncontaining the rview comments to the collection \\n    reviews.append(mydict) #  appending the comments to the \\nreview list \\nreturn render_template(\\'results.html\\', reviews=reviews) # \\nshowing the review to the user \\n\\ne)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor reviews as shown below: \\n\\n \\n\\nHome Page: \\n\\n18 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nSearch Results: \\n\\n \\n\\n \\n\\n \\n\\n5. Heroku: \\n\\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0cWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n\\n6. Heroku Basics: \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n \\n\\nhave one. \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xe2\\x80\\xa2  Double-click the installation file and the following window shall appear: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n20 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on install to complete the installation. \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\nd)  We have created a new file \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the review scrapper folder: \\n\\ni.  Remove the first_flask.app file from the directory. Resulting folder \\n\\n \\n\\nstructure: \\n\\nii.  A default route has been added to the app.py file to direct  to the home page \\n\\nwhen the application is initially invoked as shown below: \\n\\n \\n\\n@app.route(\\'/\\',methods=[\\'GET\\'])  # route to display the home \\npage \\n@cross_origin() \\ndef homePage(): \\n    return render_template(\"index.html\") \\n\\niii.  We have removed the part where we were writing to MongoDB. \\n\\nConsuming MongoDB might incur charges. So, we have removed that part.  \\n\\n \\n\\niv.   \\n\\n \\n\\n22 | P a g e  \\n\\n\\x0c8. Heroku app creation and deployment \\n\\nb.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nc.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\n \\n\\nbelow: \\n\\nd.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\ne.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\nf.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\ng.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\nh.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\ni.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\nj.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nk.  After deployment, heroku gives you the URL to hit the web API. \\nl.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n23 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nFinal Result:  \\n\\n \\n \\n\\n \\n\\n \\n \\nThank You! \\n\\n \\n\\n24 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n                                                 \\n\\n                                                                   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nReview scraper from \\nscratch till deployment \\n\\nvirat Sagar \\n\\n \\n\\n[Date] \\n\\n[Course title] \\n\\n\\x0c \\n\\nTable of Contents \\nPreface .................................................................................................................................................... 2 \\n\\nIntroduction ............................................................................................................................................ 3 \\n\\nPrerequisites ........................................................................................................................................... 5 \\n\\nPyCharm Installation ................................................................................................................. 6 \\n\\nMongoDB Installation ............................................................................................................... 8 \\n\\nStarting MongoDB  .................................................................................................................. 11 \\n\\nApplication Architecture ...................................................................................................................... 13 \\n\\nPython Implementation ....................................................................................................................... 13 \\n\\nHeroku .................................................................................................................................................. 19 \\n\\nHeroku Basics ....................................................................................................................................... 20 \\n\\nSteps before cloud deployment ........................................................................................................... 21 \\n\\nHeroku app creation and deployment ................................................................................................ 23 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nPreface \\n\\nThis book is intended towards helping all the data scientists out there. It is a step by \\nstep guide for creating a web scraper, in this case, a review scrapper right from scratch \\nand then deploying it to the heroku cloud platform. Text scrappers are extensively used \\nin the industry today for competitive pricing, market studies, customer sentiment \\nanalysis, etc. This book takes a simple example of an online cell phone purchase and \\ntries to explain the concepts simply, extensively, and thoroughly to create a review \\nscrapper right from scratch and then its deployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n1. Introduction: \\n\\nWeb Scraping(Text) \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xe2\\x80\\xa2  Web Crawling\\xe2\\x86\\x92 Accessing the webpages over the internet and pulling data from \\n\\nthem. \\n\\n\\xe2\\x80\\xa2  HTML Parsing\\xe2\\x86\\x92 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we buy a phone online? \\n\\n1.  We first look for a phone with good reviews \\n2.  We see on which website it\\xe2\\x80\\x99s available at the lowest price \\n3.  We check whether it\\xe2\\x80\\x99s  delivered in our area or not \\n4.  If everything looks good, then we buy the phone. \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nOther examples of the applications of web scrapping are: \\n\\n\\xe2\\x80\\xa2  Competitive pricing. \\n\\xe2\\x80\\xa2  Manufacturers monitor the market, whether the retailer is maintaining a minimum price \\n\\n\\xe2\\x80\\xa2  Sentiment analysis of the consumers, whether they are happy with the services and \\n\\nor not. \\n\\nproducts or not. \\n\\n\\xe2\\x80\\xa2  To aggregate news articles. \\n\\xe2\\x80\\xa2  To aggregate Marketing data. \\n\\xe2\\x80\\xa2  To gain financial insights from the market. \\n\\xe2\\x80\\xa2  To gather data for research. \\n\\xe2\\x80\\xa2  To generate marketing leads. \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  To collect trending topics by media houses. \\n\\nAnd, the list goes on.  \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of buying a phone online further and try to scrap the \\nreviews from the website about the phone that we are planning to buy. \\n\\nFor example, if we open filpkart.com and search for \\xe2\\x80\\x98iPhone\\xe2\\x80\\x99, the search result will be as \\nfollows: \\n \\n\\nThen if we click on a product link, it will take us to to the following page: \\n\\n \\n\\n \\n\\n4 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\nIf we scroll down on this page, we\\xe2\\x80\\x99ll get to see the comments posted by the customers: \\n\\n \\n\\nOur end goal is to build a web scraper that collects the reviews of a product from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xe2\\x80\\xa2  Python installed. \\n\\xe2\\x80\\xa2  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xe2\\x80\\xa2  Flask Installed. (A simple command: pip install flask) \\n\\xe2\\x80\\xa2  MongoDB installed (Explained Later). \\n\\xe2\\x80\\xa2  Basic understanding of Python and HTML. \\n\\xe2\\x80\\xa2  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n2.1  PyCharm Installation: \\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n12 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n              \\n \\n\\n \\n\\n \\n\\n4. Python Implementation: \\n\\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 on our local machines. \\n\\n \\n\\n13 | P a g e  \\n\\n\\x0c \\n\\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\xe2\\x80\\x98static\\xe2\\x80\\x99 and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 to hold \\nthe code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98css\\xe2\\x80\\x99 \\nfor keeping the stylesheets for our UI. \\n\\n3.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98flask_app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder.  \\n4.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the files: \\xe2\\x80\\x98main.css\\xe2\\x80\\x99 and \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The files are attached \\n\\nhere for reference.  \\n\\n5.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x98base.html\\xe2\\x80\\x99,\\xe2\\x80\\x99index.html\\xe2\\x80\\x99, \\n\\nand \\xe2\\x80\\x98results.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xe2\\x80\\xa2  base.html\\xe2\\x86\\x92 It acts as the common building block for the other two HTML \\n\\n \\n\\npages. \\n\\n\\xe2\\x80\\xa2  index.html\\xe2\\x86\\x92  Home page of our application. \\n\\xe2\\x80\\xa2  results.html\\xe2\\x86\\x92 Page to show the reviews for the searched keyword. \\n\\n6.   Now, the folder structure should look like: \\n\\n \\n\\n7.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\n \\n\\n14 | P a g e  \\n\\nbutton. \\n\\n \\n\\nbase.htmlindex.htmlresults.html\\x0cc)  The application now searches for reviews and shows the result on the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 \\n\\n \\n\\npage. \\n\\n8.  Understanding flask_app.py. \\n\\n \\na)  Import the necessary libraries: \\n\\nfrom flask import Flask, render_template, request,jsonify \\nfrom flask_cors import CORS,cross_origin \\nimport requests \\nfrom bs4 import BeautifulSoup as bs \\nfrom urllib.request import urlopen as uReq \\nimport pymongo \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\n\\nroute path, the control gets transferred inside the application. \\n\\n@app.route(\\'/\\',methods=[\\'POST\\',\\'GET\\']) # route with allowed \\nmethods as POST and GET \\n\\nd)  Now let\\xe2\\x80\\x99s understand the \\xe2\\x80\\x98index()\\xe2\\x80\\x99 function. \\n\\ni. \\n\\nIf the HTTP request method is POST(which is defined in index.html at \\nform submit action), then first check if the records for the searched \\nkeyword is already present in the database or not. If present, show that to \\nthe user. \\n\\nii. \\n\\ndbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \\n# opening a connection to Mongo \\ndb = dbConn[\\'crawlerDB\\'] # connecting to the database \\ncalled crawlerDB \\nreviews = db[searchString].find({}) # searching the \\ncollection with the name same as the keyword \\nif reviews.count() > 0: # if there is a collection with \\nsearched keyword and it has records in it \\n    return render_template(\\'results.html\\',reviews=reviews) \\n# show the results to user \\nIf the searched keyword doesn\\xe2\\x80\\x99t have a database entry, then the application \\ntries to fetch the details from the internet, as shown below: \\n\\nflipkart_url = \"https://www.flipkart.com/search?q=\" + \\nsearchString # preparing the URL to search the product on \\nFlipkart \\nuClient = uReq(flipkart_url) # requesting the webpage from \\nthe internet \\nflipkartPage = uClient.read() # reading the webpage \\nuClient.close() # closing the connection to the web server \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c \\n\\niii. \\n\\nflipkart_html = bs(flipkartPage, \"html.parser\") # parsing \\nthe webpage as HTML \\nOnce we have the entire HTML page, we try to get the product URL and \\nthen jump to the product page. It is similar to redirecting to the following \\npage: \\n\\nThe equivalent Python code is: \\n\\n \\n\\nproductLink = \"https://www.flipkart.com\" + \\nbox.div.div.div.a[\\'href\\'] # extracting the actual product \\nlink \\nprodRes = requests.get(productLink) # getting the product \\npage from server \\nprod_html = bs(prodRes.text, \"html.parser\") # parsing the \\nproduct page as HTML \\n\\niv. \\n\\nOn the product page, we need to find which HTML section contains the \\ncustomer comments. Let\\xe2\\x80\\x99s do inspect element(ctrl+shift+i) on the page first \\nto open the element-wise view of the HTML page. There we find the tag \\nwhich corresponds to the customer comments as shown below: \\n\\n16 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nPython code for implementing the same is: \\n\\n \\n\\ncommentboxes = prod_html.find_all(\\'div\\', {\\'class\\': \\n\"_3nrCtb\"}) # finding the HTML section containing the \\ncustomer comments \\n \\nOnce we have the list of all the comments, we now shall extract the \\ncustomer name(in grey), the rating(in green), comment heading(marked in \\nred), and the customer comment( highlighted in yellow) from the tag. \\n\\nv. \\n\\n \\n \\nThe Python code for the same is: \\n\\n \\n\\nreviews = [] # initializing an empty list for reviews \\n#  iterating over the comment section to get the details of \\nthe customer and their comments \\nfor commentbox in commentboxes: \\n\\n17 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nvi. \\n\\n    try: \\n        name = commentbox.div.div.find_all(\\'p\\', {\\'class\\': \\n\\'_3LYOAd _3sxSiS\\'})[0].text \\n \\n    except: \\n        name = \\'No Name\\' \\n \\n    try: \\n        rating = commentbox.div.div.div.div.text \\n \\n    except: \\n        rating = \\'No Rating\\' \\n \\n    try: \\n        commentHead = commentbox.div.div.div.p.text \\n    except: \\n        commentHead = \\'No Comment Heading\\' \\n    try: \\n        comtag = commentbox.div.div.find_all(\\'div\\', \\n{\\'class\\': \\'\\'}) \\n        custComment = comtag[0].div.text \\n    except: \\n        custComment = \\'No Customer Comment\\' \\nIf you notice, the parsing is done using the try-except blocks. It is done to \\nhandle the exception cases. If there is an exception in parsing the tag, we\\xe2\\x80\\x99ll \\ninsert a default string in that place. \\nOnce we have the details, we\\xe2\\x80\\x99ll insert them into MongoDB. After that, \\nwe\\xe2\\x80\\x99ll return the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 page as the response to the user containing all \\nthe reviews.  The python code for that is: \\n\\nmydict = {\"Product\": searchString, \"Name\": name, \"Rating\": \\nrating, \"CommentHead\": commentHead, \\n              \"Comment\": custComment} # saving that detail \\nto a dictionary \\n    x = table.insert_one(mydict) #insertig the dictionary \\ncontaining the rview comments to the collection \\n    reviews.append(mydict) #  appending the comments to the \\nreview list \\nreturn render_template(\\'results.html\\', reviews=reviews) # \\nshowing the review to the user \\n\\ne)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor reviews as shown below: \\n\\n \\n\\nHome Page: \\n\\n18 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nSearch Results: \\n\\n \\n\\n \\n\\n \\n\\n5. Heroku: \\n\\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0cWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n\\n6. Heroku Basics: \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n \\n\\nhave one. \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xe2\\x80\\xa2  Double-click the installation file and the following window shall appear: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n20 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on install to complete the installation. \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\nd)  We have created a new file \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the review scrapper folder: \\n\\ni.  Remove the first_flask.app file from the directory. Resulting folder \\n\\n \\n\\nstructure: \\n\\nii.  A default route has been added to the app.py file to direct  to the home page \\n\\nwhen the application is initially invoked as shown below: \\n\\n \\n\\n@app.route(\\'/\\',methods=[\\'GET\\'])  # route to display the home \\npage \\n@cross_origin() \\ndef homePage(): \\n    return render_template(\"index.html\") \\n\\niii.  We have removed the part where we were writing to MongoDB. \\n\\nConsuming MongoDB might incur charges. So, we have removed that part.  \\n\\n \\n\\niv.   \\n\\n \\n\\n22 | P a g e  \\n\\n\\x0c8. Heroku app creation and deployment \\n\\nb.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nc.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\n \\n\\nbelow: \\n\\nd.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\ne.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\nf.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\ng.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\nh.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\ni.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\nj.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nk.  After deployment, heroku gives you the URL to hit the web API. \\nl.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n23 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nFinal Result:  \\n\\n \\n \\n\\n \\n\\n \\n \\nThank You! \\n\\n \\n\\n24 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n                                                 \\n\\n                                                                   \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nReview scraper from \\nscratch till deployment \\n\\nvirat Sagar \\n\\n \\n\\n[Date] \\n\\n[Course title] \\n\\n\\x0c \\n\\nTable of Contents \\nPreface .................................................................................................................................................... 2 \\n\\nIntroduction ............................................................................................................................................ 3 \\n\\nPrerequisites ........................................................................................................................................... 5 \\n\\nPyCharm Installation ................................................................................................................. 6 \\n\\nMongoDB Installation ............................................................................................................... 8 \\n\\nStarting MongoDB  .................................................................................................................. 11 \\n\\nApplication Architecture ...................................................................................................................... 13 \\n\\nPython Implementation ....................................................................................................................... 13 \\n\\nHeroku .................................................................................................................................................. 19 \\n\\nHeroku Basics ....................................................................................................................................... 20 \\n\\nSteps before cloud deployment ........................................................................................................... 21 \\n\\nHeroku app creation and deployment ................................................................................................ 23 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\nPreface \\n\\nThis book is intended towards helping all the data scientists out there. It is a step by \\nstep guide for creating a web scraper, in this case, a review scrapper right from scratch \\nand then deploying it to the heroku cloud platform. Text scrappers are extensively used \\nin the industry today for competitive pricing, market studies, customer sentiment \\nanalysis, etc. This book takes a simple example of an online cell phone purchase and \\ntries to explain the concepts simply, extensively, and thoroughly to create a review \\nscrapper right from scratch and then its deployment to a cloud environment. \\n\\nHappy Learning! \\n\\n \\n\\n \\n\\n2 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n1. Introduction: \\n\\nWeb Scraping(Text) \\n\\nWeb scraping is a technique using which the webpages from the internet are fetched and parsed \\nto understand and extract specific information similar to a human being. Web scrapping \\nconsists of two parts: \\n\\n\\xe2\\x80\\xa2  Web Crawling\\xe2\\x86\\x92 Accessing the webpages over the internet and pulling data from \\n\\nthem. \\n\\n\\xe2\\x80\\xa2  HTML Parsing\\xe2\\x86\\x92 Parsing the HTML content of the webpages obtained through web \\n\\ncrawling and then extracting specific information from it. \\n\\nHence, web scrappers are applications/bots, which automatically send requests to websites and \\nthen extract the desired information from the website output. \\n\\nLet\\xe2\\x80\\x99s take an example:  \\n\\nhow do we buy a phone online? \\n\\n1.  We first look for a phone with good reviews \\n2.  We see on which website it\\xe2\\x80\\x99s available at the lowest price \\n3.  We check whether it\\xe2\\x80\\x99s  delivered in our area or not \\n4.  If everything looks good, then we buy the phone. \\n\\nWhat if there is a computer program that can do all of these for us? That\\xe2\\x80\\x99s what web scrappers \\nnecessarily do. They try to understand the webpage content as a human would do. \\n\\nOther examples of the applications of web scrapping are: \\n\\n\\xe2\\x80\\xa2  Competitive pricing. \\n\\xe2\\x80\\xa2  Manufacturers monitor the market, whether the retailer is maintaining a minimum price \\n\\n\\xe2\\x80\\xa2  Sentiment analysis of the consumers, whether they are happy with the services and \\n\\nor not. \\n\\nproducts or not. \\n\\n\\xe2\\x80\\xa2  To aggregate news articles. \\n\\xe2\\x80\\xa2  To aggregate Marketing data. \\n\\xe2\\x80\\xa2  To gain financial insights from the market. \\n\\xe2\\x80\\xa2  To gather data for research. \\n\\xe2\\x80\\xa2  To generate marketing leads. \\n\\n \\n\\n3 | P a g e  \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  To collect trending topics by media houses. \\n\\nAnd, the list goes on.  \\n\\nIn this document, we\\xe2\\x80\\x99ll take the example of buying a phone online further and try to scrap the \\nreviews from the website about the phone that we are planning to buy. \\n\\nFor example, if we open filpkart.com and search for \\xe2\\x80\\x98iPhone\\xe2\\x80\\x99, the search result will be as \\nfollows: \\n \\n\\nThen if we click on a product link, it will take us to to the following page: \\n\\n \\n\\n \\n\\n4 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\nIf we scroll down on this page, we\\xe2\\x80\\x99ll get to see the comments posted by the customers: \\n\\n \\n\\nOur end goal is to build a web scraper that collects the reviews of a product from the \\ninternet. \\n\\n2. Prerequisites: \\n\\nThe things needed before we start building a python based web scraper are: \\n\\n\\xe2\\x80\\xa2  Python installed. \\n\\xe2\\x80\\xa2  A Python IDE (Integrated Development Environment): like PyCharm, Spyder, or any \\n\\nother IDE of choice (Explained Later) \\n\\n\\xe2\\x80\\xa2  Flask Installed. (A simple command: pip install flask) \\n\\xe2\\x80\\xa2  MongoDB installed (Explained Later). \\n\\xe2\\x80\\xa2  Basic understanding of Python and HTML. \\n\\xe2\\x80\\xa2  Basic understanding of Git (download Git CLI from https://gitforwindows.org/ ) \\n\\n5 | P a g e  \\n\\n\\x0c \\n\\n2.1  PyCharm Installation: \\na)  Go to the link https://www.jetbrains.com/pycharm/download/#section=windows and \\n\\nb)  Double click on the installation file to start the installation process and click next to \\n\\ndownload the community edition. \\n\\ncontinue. \\n\\nc)  Select the directory to install PyCharm and then click next. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n6 | P a g e  \\n\\n\\x0c \\n\\nd)  Check the appropriate checkboxes and then click next. \\n\\ne)  Choose the name of the start menu folder and then click on install to finish the installation. \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n\\n \\n\\n7 | P a g e  \\n\\n\\x0c2.2  MongoDB Installation: \\n\\n \\n\\n \\n \\n\\n1.  Go to the page: https://www.mongodb.com/download-center/community \\nand select the MongoDB installation to download based on your operating \\nsystem. \\n\\n               \\n\\n         \\n\\n \\n\\n2.  After the installer gets downloaded, double click on the installer file to start \\n\\ninstalling the application. \\n\\n                    \\n\\n           \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n8 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n3.  Click on the next button to move to the next step and accept the agreement. \\n\\n4.  Select the type of installation: \\n\\n \\n\\n \\n\\n9 | P a g e  \\n\\n\\x0c \\n\\n5.  Select the features to install. \\n\\n6.  Click on next and then configure/customize the way you want the \\n\\napplication to be installed. \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n10 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n\\n7.  Click next and then click on install to start the MongoDB installation. \\n\\n \\n\\n2.3  Starting MongoDB: \\n\\n \\n1.  Go the services section and then start the MongoDB service if not already started. \\n\\n \\n\\n                      \\n\\n2.  Now, to check whether the database server is up or not, go to the bin directory of the \\nMongoDB installation and run the \\xe2\\x80\\x98mongo\\xe2\\x80\\x99 command as shown. If the command runs \\nsuccessfully, it means that the server is up and running, and we can proceed. \\n\\n \\n\\n \\n\\n11 | P a g e  \\n\\n\\x0c \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n12 | P a g e  \\n\\n\\x0c \\n\\n3. Application Architecture: \\n\\nThe architecture of the application is: \\n\\n              \\n \\n\\n \\n\\n \\n\\n4. Python Implementation: \\n\\nNote: I have used PyCharm as an IDE for this documentation \\n1.  Let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 on our local machines. \\n\\n \\n\\n13 | P a g e  \\n\\n\\x0c \\n\\n2.  Inside that folder, let\\xe2\\x80\\x99s create two more folders called \\xe2\\x80\\x98static\\xe2\\x80\\x99 and \\xe2\\x80\\x98templates\\xe2\\x80\\x99 to hold \\nthe code for the UI of our application. Inside \\xe2\\x80\\x98static\\xe2\\x80\\x99, let\\xe2\\x80\\x99s create a folder called \\xe2\\x80\\x98css\\xe2\\x80\\x99 \\nfor keeping the stylesheets for our UI. \\n\\n3.  Let\\xe2\\x80\\x99s create a file called \\xe2\\x80\\x98flask_app.py\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder.  \\n4.  Inside the folder \\xe2\\x80\\x98css\\xe2\\x80\\x99, create the files: \\xe2\\x80\\x98main.css\\xe2\\x80\\x99 and \\xe2\\x80\\x98style.css\\xe2\\x80\\x99. The files are attached \\n\\nhere for reference.  \\n\\n5.  Inside the folder \\xe2\\x80\\x98templates\\xe2\\x80\\x99, create three HTML files called: \\xe2\\x80\\x98base.html\\xe2\\x80\\x99,\\xe2\\x80\\x99index.html\\xe2\\x80\\x99, \\n\\nand \\xe2\\x80\\x98results.html\\xe2\\x80\\x99. The files are attached here for reference.  \\n\\n \\n\\n\\xe2\\x80\\xa2  base.html\\xe2\\x86\\x92 It acts as the common building block for the other two HTML \\n\\n \\n\\npages. \\n\\n\\xe2\\x80\\xa2  index.html\\xe2\\x86\\x92  Home page of our application. \\n\\xe2\\x80\\xa2  results.html\\xe2\\x86\\x92 Page to show the reviews for the searched keyword. \\n\\n6.   Now, the folder structure should look like: \\n\\n \\n\\n7.  Now, let\\xe2\\x80\\x99s understand the flow: \\n\\na)  When the application starts, the user sees the page called \\xe2\\x80\\x98index.html\\xe2\\x80\\x99. \\nb)  The user enters the search keyword into the search box and presses the submit \\n\\n \\n\\n14 | P a g e  \\n\\nbutton. \\n\\n \\n\\nbase.htmlindex.htmlresults.html\\x0cc)  The application now searches for reviews and shows the result on the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 \\n\\n \\n\\npage. \\n\\n8.  Understanding flask_app.py. \\n\\n \\na)  Import the necessary libraries: \\n\\nfrom flask import Flask, render_template, request,jsonify \\nfrom flask_cors import CORS,cross_origin \\nimport requests \\nfrom bs4 import BeautifulSoup as bs \\nfrom urllib.request import urlopen as uReq \\nimport pymongo \\n  \\n\\nb)  Initialize the flask app \\n\\napp = Flask(__name__)  # initialising the flask app with the name \\n\\'app\\' \\n \\n\\nc)  Creating the routes to redirect the control inside the application itself. Based on the \\n\\nroute path, the control gets transferred inside the application. \\n\\n@app.route(\\'/\\',methods=[\\'POST\\',\\'GET\\']) # route with allowed \\nmethods as POST and GET \\n\\nd)  Now let\\xe2\\x80\\x99s understand the \\xe2\\x80\\x98index()\\xe2\\x80\\x99 function. \\n\\ni. \\n\\nIf the HTTP request method is POST(which is defined in index.html at \\nform submit action), then first check if the records for the searched \\nkeyword is already present in the database or not. If present, show that to \\nthe user. \\n\\nii. \\n\\ndbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  \\n# opening a connection to Mongo \\ndb = dbConn[\\'crawlerDB\\'] # connecting to the database \\ncalled crawlerDB \\nreviews = db[searchString].find({}) # searching the \\ncollection with the name same as the keyword \\nif reviews.count() > 0: # if there is a collection with \\nsearched keyword and it has records in it \\n    return render_template(\\'results.html\\',reviews=reviews) \\n# show the results to user \\nIf the searched keyword doesn\\xe2\\x80\\x99t have a database entry, then the application \\ntries to fetch the details from the internet, as shown below: \\n\\nflipkart_url = \"https://www.flipkart.com/search?q=\" + \\nsearchString # preparing the URL to search the product on \\nFlipkart \\nuClient = uReq(flipkart_url) # requesting the webpage from \\nthe internet \\nflipkartPage = uClient.read() # reading the webpage \\nuClient.close() # closing the connection to the web server \\n\\n15 | P a g e  \\n\\n \\n\\n\\x0c \\n\\niii. \\n\\nflipkart_html = bs(flipkartPage, \"html.parser\") # parsing \\nthe webpage as HTML \\nOnce we have the entire HTML page, we try to get the product URL and \\nthen jump to the product page. It is similar to redirecting to the following \\npage: \\n\\nThe equivalent Python code is: \\n\\n \\n\\nproductLink = \"https://www.flipkart.com\" + \\nbox.div.div.div.a[\\'href\\'] # extracting the actual product \\nlink \\nprodRes = requests.get(productLink) # getting the product \\npage from server \\nprod_html = bs(prodRes.text, \"html.parser\") # parsing the \\nproduct page as HTML \\n\\niv. \\n\\nOn the product page, we need to find which HTML section contains the \\ncustomer comments. Let\\xe2\\x80\\x99s do inspect element(ctrl+shift+i) on the page first \\nto open the element-wise view of the HTML page. There we find the tag \\nwhich corresponds to the customer comments as shown below: \\n\\n16 | P a g e  \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nPython code for implementing the same is: \\n\\n \\n\\ncommentboxes = prod_html.find_all(\\'div\\', {\\'class\\': \\n\"_3nrCtb\"}) # finding the HTML section containing the \\ncustomer comments \\n \\nOnce we have the list of all the comments, we now shall extract the \\ncustomer name(in grey), the rating(in green), comment heading(marked in \\nred), and the customer comment( highlighted in yellow) from the tag. \\n\\nv. \\n\\n \\n \\nThe Python code for the same is: \\n\\n \\n\\nreviews = [] # initializing an empty list for reviews \\n#  iterating over the comment section to get the details of \\nthe customer and their comments \\nfor commentbox in commentboxes: \\n\\n17 | P a g e  \\n\\n \\n\\n\\x0c \\n\\nvi. \\n\\n    try: \\n        name = commentbox.div.div.find_all(\\'p\\', {\\'class\\': \\n\\'_3LYOAd _3sxSiS\\'})[0].text \\n \\n    except: \\n        name = \\'No Name\\' \\n \\n    try: \\n        rating = commentbox.div.div.div.div.text \\n \\n    except: \\n        rating = \\'No Rating\\' \\n \\n    try: \\n        commentHead = commentbox.div.div.div.p.text \\n    except: \\n        commentHead = \\'No Comment Heading\\' \\n    try: \\n        comtag = commentbox.div.div.find_all(\\'div\\', \\n{\\'class\\': \\'\\'}) \\n        custComment = comtag[0].div.text \\n    except: \\n        custComment = \\'No Customer Comment\\' \\nIf you notice, the parsing is done using the try-except blocks. It is done to \\nhandle the exception cases. If there is an exception in parsing the tag, we\\xe2\\x80\\x99ll \\ninsert a default string in that place. \\nOnce we have the details, we\\xe2\\x80\\x99ll insert them into MongoDB. After that, \\nwe\\xe2\\x80\\x99ll return the \\xe2\\x80\\x98results.html\\xe2\\x80\\x99 page as the response to the user containing all \\nthe reviews.  The python code for that is: \\n\\nmydict = {\"Product\": searchString, \"Name\": name, \"Rating\": \\nrating, \"CommentHead\": commentHead, \\n              \"Comment\": custComment} # saving that detail \\nto a dictionary \\n    x = table.insert_one(mydict) #insertig the dictionary \\ncontaining the rview comments to the collection \\n    reviews.append(mydict) #  appending the comments to the \\nreview list \\nreturn render_template(\\'results.html\\', reviews=reviews) # \\nshowing the review to the user \\n\\ne)  After this, we\\xe2\\x80\\x99ll just run our python app on our local system, and it\\xe2\\x80\\x99ll start scraping \\n\\nfor reviews as shown below: \\n\\n \\n\\nHome Page: \\n\\n18 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\nSearch Results: \\n\\n \\n\\n \\n\\n \\n\\n5. Heroku: \\n\\nThe Python app that we have developed is residing on our local machine. But to make it \\navailable to end-users,  we need to deploy it to either an on-premise server or to a cloud \\nservice. Heroku is one such cloud service provider. It is free to use(till 5 applications). \\n\\n \\n\\n19 | P a g e  \\n\\n\\x0cWe\\xe2\\x80\\x99ll deploy this application to the Heroku cloud, and then anybody with the URL can then \\nconsume our app. \\n\\n6. Heroku Basics: \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll first go to heroku.com, and we\\xe2\\x80\\x99ll create a new account if we already don\\xe2\\x80\\x99t \\n\\n \\n\\nhave one. \\n\\n\\xe2\\x80\\xa2  We\\xe2\\x80\\x99ll download and install the Heroku CLI from the Heroku website: \\n\\nhttps://devcenter.heroku.com/articles/heroku-cli. \\n\\n\\xe2\\x80\\xa2  Double-click the installation file and the following window shall appear: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on next and select the installation directory for the CLI. \\n\\n \\n\\n20 | P a g e  \\n\\n\\x0c \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Click on install to complete the installation. \\n\\n \\n\\n7. Steps before cloud deployment: \\n\\n              We need to change our code a bit so that it works unhindered on the cloud, as well. \\n\\na)  Add a file called \\xe2\\x80\\x98gitignore\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the list \\nof the files which we don\\xe2\\x80\\x99t want to include in the git repository. My gitignore file looks \\nlike: \\n\\n.idea \\nAs I am using PyCharm as an IDE, and it\\xe2\\x80\\x99s provided by the Intellij Idea \\ncommunity, it automatically adds the .idea folder containing some metadata. We \\nneed not include them in our cloud app. \\n\\nb)  Add a file called \\xe2\\x80\\x98Procfile\\xe2\\x80\\x99 inside the \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. This folder contains the \\n\\ncommand to run the flask application once deployed on the server: \\n\\nweb: gunicorn app:app \\nHere, the keyword \\xe2\\x80\\x98web\\xe2\\x80\\x99 specifies that the application is a web application. And the \\npart \\xe2\\x80\\x98app:app\\xe2\\x80\\x99 instructs the program to look for a flask application called \\xe2\\x80\\x98app\\xe2\\x80\\x99 \\ninside the \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 file. Gunicorn is a Web Server Gateway Interface (WSGI) \\nHTTP server for Python. \\n\\nc)  Open a command prompt window and navigate to your \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. Enter the \\ncommand \\xe2\\x80\\x98pip freeze > requirements.txt\\xe2\\x80\\x99. This command generates the \\xe2\\x80\\x98requirements.txt\\xe2\\x80\\x99 \\nfile. My requirements.txt looks like: \\n\\n \\n\\n21 | P a g e  \\n\\n\\x0c \\n\\nbeautifulsoup4==4.8.1 \\nbs4==0.0.1 \\ncertifi==2019.9.11 \\nClick==7.0 \\nFlask==1.1.1 \\nFlask-Cors==3.0.8 \\ngunicorn==20.0.4 \\nitsdangerous==1.1.0 \\nJinja2==2.10.3 \\nMarkupSafe==1.1.1 \\nnumpy==1.17.4 \\nopencv-python==4.1.2.30 \\nPillow==6.2.1 \\npymongo==3.9.0 \\nrequests==2.21.0 \\nrequests-oauthlib==1.2.0 \\nsix==1.13.0 \\nsoupsieve==1.9.5 \\nWerkzeug==0.16.0 \\nrequirements.txt helps the Heroku cloud app to install all the dependencies before starting \\nthe webserver. \\n\\nd)  We have created a new file \\xe2\\x80\\x98app.py\\xe2\\x80\\x99 inside the review scrapper folder: \\n\\ni.  Remove the first_flask.app file from the directory. Resulting folder \\n\\n \\n\\nstructure: \\n\\nii.  A default route has been added to the app.py file to direct  to the home page \\n\\nwhen the application is initially invoked as shown below: \\n\\n \\n\\n@app.route(\\'/\\',methods=[\\'GET\\'])  # route to display the home \\npage \\n@cross_origin() \\ndef homePage(): \\n    return render_template(\"index.html\") \\n\\niii.  We have removed the part where we were writing to MongoDB. \\n\\nConsuming MongoDB might incur charges. So, we have removed that part.  \\n\\n \\n\\niv.   \\n\\n \\n\\n22 | P a g e  \\n\\n\\x0c8. Heroku app creation and deployment \\n\\nb.  After installing the Heroku CLI, Open a command prompt window and navigate to \\n\\nyour \\xe2\\x80\\x98reviewScrapper\\xe2\\x80\\x99 folder. \\n\\nc.  Type the command \\xe2\\x80\\x98heroku login\\xe2\\x80\\x99 to login to your heroku account as shown   \\n\\n \\n\\nbelow: \\n\\nd.  After logging in to Heroku, enter the command \\xe2\\x80\\x98heroku create\\xe2\\x80\\x99 to create a \\n\\nheroku app. It will give you the URL of your Heroku app after successful creation. \\ne.  Before deploying the code to the Heroku cloud, we need to commit the changes to \\n\\n \\n\\nthe local git repository. \\n\\nf.  Type the command \\xe2\\x80\\x98git init to initialize a local git repository  as shown below: \\n\\ng.  Enter the command \\xe2\\x80\\x98git status\\xe2\\x80\\x99 to see the uncommitted changes \\nh.  Enter the command \\xe2\\x80\\x98git add .\\xe2\\x80\\x99 to add the uncommitted changes to the local \\n\\n \\n\\ni.  Enter the command \\xe2\\x80\\x98git commit -am \"make it better\"\\xe2\\x80\\x99 to commit the changes \\n\\nj.  Enter the command \\xe2\\x80\\x98git push heroku master\\xe2\\x80\\x99 to push the code to the heroku \\n\\nk.  After deployment, heroku gives you the URL to hit the web API. \\nl.  Once your application is deployed successfully, enter the command \\xe2\\x80\\x98heroku logs \\n\\nrepository. \\n\\nto the local repository. \\n\\ncloud. \\n\\n--tail\\xe2\\x80\\x99 to see the logs. \\n\\n23 | P a g e  \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nFinal Result:  \\n\\n \\n \\n\\n \\n\\n \\n \\nThank You! \\n\\n \\n\\n24 | P a g e  \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "b' \\n\\n \\n\\n \\n\\nWorking with MySQL and Python \\n\\nInstalling MySQL Workbench \\n\\n\\xe2\\x80\\xa2  Download MYSQL application. Go to page : https://dev.mysql.com/downloads/installer/ \\n\\xe2\\x80\\xa2  Click on the link highlighted below. \\n\\n\\xe2\\x80\\xa2  Once downloaded, install the application. Follow the instructions and continue with \\n\\ninstallation. \\n\\n \\n\\n \\n\\n\\x0c\\xe2\\x80\\xa2  Select Standalone MySQL server option \\n\\n\\xe2\\x80\\xa2  Keep the default values for \\xe2\\x80\\x9cType and Networking\\xe2\\x80\\x9d section \\n\\n \\n\\n \\n\\n\\x0c\\xe2\\x80\\xa2  Finish the installation by pressing next. \\n\\n\\xe2\\x80\\xa2  Once the installation is complete, open the MYSQL workbench and click on the local instance \\n\\nconnection. The window will look something like this: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Provide the password that you setup during installation: \\n\\nRemember the user name mentioned above as we will need it to connect python to Mysql. \\n\\n\\xe2\\x80\\xa2  Once the connection is established, you will see a window like this: \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\nConnecting MySql With Python \\n\\nLet\\xe2\\x80\\x99s use python to connect with this database. \\n\\nMysql. \\n\\n\\xe2\\x80\\x9cpip install mysql-conector-python\\xe2\\x80\\x9d \\n\\n\\xe2\\x80\\xa2  First we need to install \\xe2\\x80\\x9cmysql-connector-python\\xe2\\x80\\x9d package to establish a connection with \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Once the package is installed, we can go ahead with establishing the connection. \\n\\n\\xe2\\x80\\xa2  Enter the details as shown above, your mysql server is running locally so host is \\xe2\\x80\\x9clocalhost\\xe2\\x80\\x9d, \\n\\n               enter the username and password as was setup during installation. \\n\\nNote: \\xe2\\x80\\x9cuse_pure\\xe2\\x80\\x9d argument forces mysqlConnector to user pure python connection \\ninstead of C extensions which leads to SSL error. \\n\\n\\xe2\\x80\\xa2  To check if the connection is established, we can use print (mydb.is_connected). It will \\n\\nreturn TRUE if the connection is established else FALSE. \\n\\n\\x0c\\xe2\\x80\\xa2  Our connection is established now. \\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s start with creating a database with name Student. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s check if database is created in our MySQL Workbench. \\n\\n\\xe2\\x80\\xa2  Great! Database is created.  \\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s start with creating tables. Let\\xe2\\x80\\x99s first connect to the created database in our workbench \\nso that we can view the tables, once we create them from python. \\n\\n \\n\\n\\xe2\\x80\\xa2  Give the database name: \\n\\n \\n\\n \\n\\n\\x0c\\xe2\\x80\\xa2  Now we are connected to the created database. Let\\xe2\\x80\\x99s start with creating tables. \\n\\n\\xe2\\x80\\xa2  We need to pass an additional parameter, database name, while connecting to server. We \\n\\nhave passed \\xe2\\x80\\x9cStudent\\xe2\\x80\\x9d database in which we are going to create the table. \\n \\nLet\\xe2\\x80\\x99s see if the table is connected in our Mysql. \\n\\n\\xe2\\x80\\xa2 \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2  Our table is now created in the mentioned database. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s start with inserting values in our table: \\n\\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s check if the values are inserted: \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\nLet\\xe2\\x80\\x99s now insert values into a new table in a new database from a file. \\n\\n\\xe2\\x80\\xa2 \\n\\xe2\\x80\\xa2  We are loading all the values in the file \\xe2\\x80\\x9cglass. Data\\xe2\\x80\\x9d into our table. \\n\\xe2\\x80\\xa2  We created a new database named \\xe2\\x80\\x9cGlass Data\\xe2\\x80\\x9d and a new table \\xe2\\x80\\x9cGlass Data\\xe2\\x80\\x9d in it. \\n\\xe2\\x80\\xa2  We are reading each row from the file and inserting into the table. \\n\\n \\n\\n \\n\\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s check if the values are inserted in the table. \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2  The values are inserted. \\n\\nLet\\xe2\\x80\\x99s see some other commands: \\n\\n1)  Selecting from table \\n\\nResult: \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s try to store all the select values into a Dataframe. \\n\\nWe can use pandas.read_sql  to store the values in a dataframe. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0cLet\\xe2\\x80\\x99s see the result: \\n\\n \\n\\n \\n\\n2)  Update statement \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s check in MySql workbench: \\n\\n3)  Delete statement \\n\\n \\n\\n \\n\\n \\n\\n\\x0c\\xe2\\x80\\xa2 \\n\\nLet\\xe2\\x80\\x99s check in MySql workbench: \\n\\n \\n\\n4)  Group by, Order by \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c'-------------this is the end of file----------------------------\n",
      "\n",
      "merged file for D:\\ drive created at location: D:\\xyz1.txt\n",
      "files found in E:\\ are follwoing:  \n",
      "\n",
      "merged file for E:\\ drive created at location: E:\\xyz1.txt\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import os\n",
    "\n",
    "\n",
    "#function to display list of files which matches search command\n",
    "def showlist_files():\n",
    "    \"\"\"funtion to show the list of\n",
    "    files\"\"\"\n",
    "    inp = txtfld1.get()\n",
    "    def find_files(filename, search_path):\n",
    "        result = []\n",
    "        for root, dir, files in os.walk(search_path):\n",
    "            if filename in files:\n",
    "                result.append(os.path.join(root, filename))\n",
    "            else:\n",
    "                for i in files:\n",
    "                    if filename==i.split(\".\")[0]:\n",
    "                        result.append(os.path.join(root, i))\n",
    "        return result\n",
    "    lis=[\"D:\\\\\",\"E:\\\\\"]\n",
    "    for i in lis:\n",
    "        s=find_files(inp,i)\n",
    "        print(f\"files found in {i} are follwoing:  \")\n",
    "        for i in s:\n",
    "            print(i)\n",
    "        for i in s:\n",
    "            op1.insert(END, i)\n",
    "            op1.insert(END, \"\\n\")\n",
    "            \n",
    "            \n",
    "#function to display list of files and merge files of same extension and save in a new file of name xyz1.txt \n",
    "def showlist_ext():\n",
    "    \"\"\"function to display and merge files\n",
    "        of same extensions\"\"\"\n",
    "    inp = txtfld2.get()\n",
    "    inp1=inp[1:]\n",
    "    import os\n",
    "    def find_files1(filename, search_path):\n",
    "        result = []\n",
    "        for root, dir, files in os.walk(search_path):\n",
    "            for i in files:\n",
    "                    try:\n",
    "                        a=i.split(\".\")[1]\n",
    "                        if filename==a:\n",
    "                            result.append(os.path.join(root, i))\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        return result\n",
    "    lis=[\"D:\\\\\",\"E:\\\\\"]\n",
    "    for i in lis:\n",
    "        s=find_files1(inp1,i)\n",
    "        print(f\"files found in {i} are follwoing:  \")\n",
    "        for j in s:\n",
    "            print(j)\n",
    "        for k in s:\n",
    "            op2.insert(END, k)\n",
    "            op2.insert(END, \"\\n\")\n",
    "        f1=open(f\"{i}xyz1.txt\",\"w\")\n",
    "        for l in s:\n",
    "            if inp1==\"pdf\":\n",
    "                from pdfminer.high_level import extract_text\n",
    "                text = extract_text(l)\n",
    "                text1=str(text.encode(\"utf-8\"))\n",
    "                f1.write(text1)\n",
    "                f1.write(f\"-------------this is the end of file----------------------------\")\n",
    "                f1.write(\"\\n\")\n",
    "            else:\n",
    "                f=open(l ,\"r\", errors=\"ignore\")\n",
    "                a=f.read()\n",
    "                f1.write(a)\n",
    "                f1.write(f\"-------------this is the end of file----------------------------\")\n",
    "                f1.write(\"\\n\")\n",
    "                f.close()\n",
    "        f1.close()\n",
    "        f1=open(f\"{i}xyz1.txt\",\"r\")\n",
    "        a=f1.read()\n",
    "        print(a)\n",
    "        print(f\"merged file for {i} drive created at location: {i}xyz1.txt\")\n",
    "        op3.insert(END, f\"merged file for {i} drive created at location: {i}xyz1.txt\")\n",
    "        op3.insert(END, \"\\n\")\n",
    "        f1.close()\n",
    "            \n",
    "#defining buttons,lables,text box and entry box in UI Window           \n",
    "window=Tk()\n",
    "\n",
    "btn1=Button(window, text=\"SEARCH\", fg='blue', command=showlist_files)\n",
    "btn1.place(x=350, y=100)\n",
    "\n",
    "btn2=Button(window, text=\"MERGE\", fg='blue', command=showlist_ext)\n",
    "btn2.place(x=350, y=320)\n",
    "\n",
    "op1= Text(window,height = 5, width = 100, bd = 7)\n",
    "op1.place(x=10, y=140)\n",
    "\n",
    "op2= Text(window,height = 5, width = 100, bd = 7)\n",
    "op2.place(x=10, y=370)\n",
    "\n",
    "op3= Text(window,height = 5, width = 70, bd = 7)\n",
    "op3.place(x=10, y=500)\n",
    "\n",
    "lbl3=Label(window, text=\"Merged file location for different drives..\", fg='red', font=(\"Helvetica\", 10))\n",
    "lbl3.place(x=10, y=470)\n",
    "\n",
    "lbl1=Label(window, text=\"Search for files with or without extension ex. xyz or xyz.txt\", fg='red', font=(\"Helvetica\", 10))\n",
    "lbl1.place(x=60, y=50)\n",
    "\n",
    "lbl2=Label(window, text=\"Search files for extension ex. .txt , .pdf \", fg='red', font=(\"Helvetica\", 10))\n",
    "lbl2.place(x=60, y=280)\n",
    "\n",
    "txtfld1=Entry(window, text=\"enter the file name with or without extension\", bd=7)\n",
    "txtfld1.place(x=100, y=100)\n",
    "\n",
    "txtfld2=Entry(window, text=\"enter the extension with .\", bd=7)\n",
    "txtfld2.place(x=100, y=320)\n",
    "\n",
    "window.title('DESKTOP SEARCH APP')\n",
    "window.geometry(\"1000x600+10+10\")\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are getting this error:\n",
    "IOPub data rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_data_rate_limit`. \n",
    "Current values: NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec) NotebookApp.rate_limit_window=3.0 (secs)\n",
    "\n",
    "Use this command in prompt to open notebook:\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
